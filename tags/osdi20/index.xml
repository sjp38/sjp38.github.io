<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>osdi20 | hacklog</title>
    <link>/tags/osdi20/</link>
      <atom:link href="/tags/osdi20/index.xml" rel="self" type="application/rss+xml" />
    <description>osdi20</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 19 Dec 2020 22:35:27 +0100</lastBuildDate>
    <image>
      <url>/img/tux.png</url>
      <title>osdi20</title>
      <link>/tags/osdi20/</link>
    </image>
    
    <item>
      <title>Watching OSDI 2020 presentation videos</title>
      <link>/post/osdi2020_videos_review/</link>
      <pubDate>Sat, 19 Dec 2020 22:35:27 +0100</pubDate>
      <guid>/post/osdi2020_videos_review/</guid>
      <description>&lt;p&gt;I set watching at least one OSDI&#39;20 presentation video per day during the
long vacation as one of my plans.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;a-large-scale-analysis-of-hundreds-of-in-memory-cache-clusters-at-twitter&#34;&gt;A large scale analysis of hundreds of in-memory cache clusters at Twitter&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=OQtMM5vdhlI&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=OQtMM5vdhlI&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The authors traced and analyzed the workloads on the Twitter&amp;rsquo;s in-memory cache
systems.  To me, below findings were interesting.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are many write requests.&lt;/li&gt;
&lt;li&gt;Size of each object is not so big (median 200 bytes), so metadata, which
sizes 64 bytes per each object, is a burden&lt;/li&gt;
&lt;li&gt;Size of each key is not small compared to the size of each value.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The trace data is available via github[1].  It&amp;rsquo;s 2.8TB for compressed version,
and 14TB for uncompressed version.&lt;/p&gt;
&lt;p&gt;[1] &lt;a href=&#34;https://github.com/twitter/cache-trace&#34;&gt;https://github.com/twitter/cache-trace&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;aifm-high-performance-application-integrated-far-memory&#34;&gt;AIFM: High-Performance, Application-Integrated Far Memory&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=xHhaniGXTUg&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=xHhaniGXTUg&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;OS-level fast-network-based far memory approaches provide transparency to the
application but wastes performance due to the semantic gap due to the
fixed-size page abstraction and the kernel space time consumption.  AIFM solves
the semantic gap using new data structure abstraction and provides user space
runtime that don&amp;rsquo;t need kernel space time consumption.  As a result, the
application is required to be modified, but the authors argue it&amp;rsquo;s only modest
change.  Compared to other state-of-the-art (FastSwap from EuroSys&#39;20), it
achieved 13x speedup.&lt;/p&gt;
&lt;h2 id=&#34;linnos-predictability-on-unpredictable-flash-storage-with-a-light-neural-network&#34;&gt;LinnOS: Predictability on Unpredictable Flash Storage with a Light Neural Network&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=yzv9lcjxhAg&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=yzv9lcjxhAg&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s not easy to predict each I/O speed of modern fast storage devices, due to
their complex internals like the caching, the wear-leveling and the garbage
collection.  One well-known solution is hedging.  It prepare an array of SSDs,
issue I/O to one of them, and if the response doesn&amp;rsquo;t made until a timeout,
revoke the request and try with another SSD.  The wait time limit bounds the
latency.  LinnOS uses an approach similar to the hedging, but it uses a neural
network that can predict if each I/O to each SSD will be served fast or not.
For this, the neural network receives current I/O queue depth and queue depths
and latencies of last few I/Os as input.  Then, it predicts if the latency will
be only fast or slow.  To mitigate with the effect from wrong predictions, it
uses biased learning and adaptive hedging based on the prediction accuracy.
The network is learned offline.&lt;/p&gt;
&lt;h2 id=&#34;do-os-abstractions-make-sense-on-fpgas&#34;&gt;Do OS abstractions make sense on FPGAs?&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=k-cp4U3JKug&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=k-cp4U3JKug&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Computer systems embedding FPGA in addition to CPU, called heterogeneous
computing systems, are widesparead nowadays.  These systems typically have high
operational complexity.  Especially developing and deploying FPGA application
is quite complicated.  For the reason, some manufacturers provide some FPGA
shells providing some of the OS abstractions.  The authors developed a FPGA
shell called Coyote, which provides full abstraction sets of the OS and
experimented if it works well for the systems.  The result was very positive.&lt;/p&gt;
&lt;h2 id=&#34;fast-rdma-based-ordered-key-value-store-using-remote-learned-cache&#34;&gt;Fast RDMA-based Ordered Key-Value Store using Remote Learned Cache&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Qv-0YL_SII4&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=Qv-0YL_SII4&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Because server-centric key-value store (KVS) performance bound to the server
CPUs, RDMA-based approaches were proposed.  Because one RTT is required for one
RDMA, the approaches require many round trips and therefore doesn&amp;rsquo;t show high
performance.  Caching the index in client is one solution, but it makes huge
client memory footage.  The authors propose to use neural network as the index
cache.  They train neural network to receive key and provide logical address of
the value for the key in server.  The network retrained for dynamci updates and
copyied to clients.  Then, client use the nerual network, which is much smaller
than the full index tree to know the address of the value and fetch it via the
RDMA.&lt;/p&gt;
&lt;h2 id=&#34;a-simpler-and-faster-nic-driver-model-for-network-functions&#34;&gt;A Simpler and Faster NIC Driver Model for Network Functions&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=zKJIY4vbBDA&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=zKJIY4vbBDA&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Software-defined network approach, which implements functionality of appliances
like bridege, router, firewall is widespread.  For better performance and
flexibility, the software complexity has increased so that it&amp;rsquo;s hard to do
formal verification of the network stacks.  The authors argue that by
sacrificing some of the fliexibility, it&amp;rsquo;s available to implement simple and
fast network stack.  They prove their idea with a new network driver model,
tinynf.  They implemented a driver for Intel 82599 based on the driver model
with only 550 lines of code.  It was able to finish formal verification 7x
faster than a state-of-the-art driver.  Also, it achieved 1.6x performance
compared to the state-of-the-art.&lt;/p&gt;
&lt;h2 id=&#34;theseus-an-experiment-in-operating-system-structure-and-state-management&#34;&gt;Theseus: an experiment in operating system structure and state management&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=i1pLDZKtlBI&#34;&gt;https://www.youtube.com/watch?v=i1pLDZKtlBI&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this paper, the authors present an OS called Theseus that designed for state
spill problem.  For this, they made it to composed with many tiny components
called &amp;lsquo;cell&amp;rsquo;, and applied rust-like language level safe guaranteeness
mechanisms in the OS level.&lt;/p&gt;
&lt;h2 id=&#34;specification-and-verification-in-the-field-applying-formal-methods-to-bpf-just-in-time-compilers-in-the-linux-kernel&#34;&gt;Specification and verification in the field: Applying formal methods to BPF just-in-time compilers in the Linux kernel&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=2V3ts5-W_9g&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=2V3ts5-W_9g&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;BPF program is verified before run, but it runs after compiled via JIT, after
the verification.  Therefore, if there is a bug in the JIT, real problem
occurs.  The authors of this paper devloped a new BPF JIT for RISC-V with a JIT
correctness specification framework, jitterbug.  The changes made to the kernel
are merged into the mainline kernel.&lt;/p&gt;
&lt;h2 id=&#34;storage-systems-are-distributed-systems-so-verify-them-that-way&#34;&gt;Storage Systems are Distributed Systems (So Verify Them That Way!)&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=4s8EeXcu_8Y&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=4s8EeXcu_8Y&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Storage systems have high complexity, so it&amp;rsquo;s hard to do the verification.  The
authors of this paper realized the architecture and charactersitics of the
storage systems are similar to those of distributed systems, and applied the
verification methodologies for distributed systems to the storage systems after
making it more general.  Based on this, they implemented a verifiable key-value
storage, VeriSafeKV.  The performance of it was similar to unverified DB
systems.&lt;/p&gt;
&lt;h2 id=&#34;caladan-mitigating-interference-at-microsecond-timescales&#34;&gt;Caladan: Mitigating Interference at Microsecond Timescales&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=G-v3ndwixOI&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=G-v3ndwixOI&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;On a system, multiple workloads that has different requirements co-scheduled.
For example, some workloads only need best-effort resources while others
require strict tail latency.  Because the workloads share some hardware
resources like LLC and memory bandwidth, it&amp;rsquo;s hard to fulfill all the
requirements.  Well know solution is resource partitioning.  Because static
partitioning could result in low resource utilization, dynamic partitioning
solutions were previously proposed.  However, the dynamic partitioning doesn&amp;rsquo;t
provide microsecond granularity decision, while 100 microseconds is the
marginal timewindow to guarantee the tail latency problem, according to the
authors&amp;rsquo; arguments.  The authors propose to use different interference signals
and design the system to work in micro-second level.&lt;/p&gt;
&lt;h2 id=&#34;semeru-a-memory-disaggregated-managed-runtime&#34;&gt;Semeru: A Memory-Disaggregated Managed Runtime&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=MFA3MmNDKaM&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=MFA3MmNDKaM&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Constructing system as distributed machines for different works such as CPU
works and memory works are know ans resource-disaggregated architecture and
gaining popularity.  However, the architecture is usually written for native
applications rather than GC-based applications.  This paper introduces JVM
runtime, Semeru, which is designed for resource-disaggregation architecture.&lt;/p&gt;
&lt;h2 id=&#34;panic-a-high-performance-programmable-nic-for-multi-tenant-networks&#34;&gt;PANIC: A High-Performance Programmable NIC for Multi-tenant Networks&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=EB6dK3L8Jzg&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=EB6dK3L8Jzg&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Multi-tenant networks require generality, flexible chaining, isolation, and
performance to programmable NIC.  However, current programmable NICs doesn&amp;rsquo;t
support those.  Authors propose a new programmable NIC design for the
requirements and introduce an implementation on FPGA, PANIC.&lt;/p&gt;
&lt;h2 id=&#34;serving-dnns-like-clockwork-performance-predictability-from-the-bottom-up&#34;&gt;Serving DNNs like Clockwork: Performance Predictability from the Bottom Up&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=wHOpY_MY57Y&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=wHOpY_MY57Y&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Nowadays, data center serves for many machine learning models for different
users.  Even though the latency of each inference is predictable because it has
no conditional branches, the models serving system could result in
unpredictable end-to-end latency.  This paper proposes a new machine learning
models serving system that designed do protect the latency.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
