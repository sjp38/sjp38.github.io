<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>osdi | hacklog</title>
    <link>/tags/osdi/</link>
      <atom:link href="/tags/osdi/index.xml" rel="self" type="application/rss+xml" />
    <description>osdi</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 19 Dec 2020 22:35:27 +0100</lastBuildDate>
    <image>
      <url>/img/tux.png</url>
      <title>osdi</title>
      <link>/tags/osdi/</link>
    </image>
    
    <item>
      <title>Watching OSDI 2020 presentation videos</title>
      <link>/post/osdi2020_videos_review/</link>
      <pubDate>Sat, 19 Dec 2020 22:35:27 +0100</pubDate>
      <guid>/post/osdi2020_videos_review/</guid>
      <description>&lt;p&gt;I set watching at least one OSDI&#39;20 presentation video per day during the
long vacation as one of my plans.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;a-large-scale-analysis-of-hundreds-of-in-memory-cache-clusters-at-twitter&#34;&gt;A large scale analysis of hundreds of in-memory cache clusters at Twitter&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=OQtMM5vdhlI&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=OQtMM5vdhlI&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The authors traced and analyzed the workloads on the Twitter&amp;rsquo;s in-memory cache
systems.  To me, below findings were interesting.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are many write requests.&lt;/li&gt;
&lt;li&gt;Size of each object is not so big (median 200 bytes), so metadata, which
sizes 64 bytes per each object, is a burden&lt;/li&gt;
&lt;li&gt;Size of each key is not small compared to the size of each value.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The trace data is available via github[1].  It&amp;rsquo;s 2.8TB for compressed version,
and 14TB for uncompressed version.&lt;/p&gt;
&lt;p&gt;[1] &lt;a href=&#34;https://github.com/twitter/cache-trace&#34;&gt;https://github.com/twitter/cache-trace&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;aifm-high-performance-application-integrated-far-memory&#34;&gt;AIFM: High-Performance, Application-Integrated Far Memory&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=xHhaniGXTUg&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=xHhaniGXTUg&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;OS-level fast-network-based far memory approaches provide transparency to the
application but wastes performance due to the semantic gap due to the
fixed-size page abstraction and the kernel space time consumption.  AIFM solves
the semantic gap using new data structure abstraction and provides user space
runtime that don&amp;rsquo;t need kernel space time consumption.  As a result, the
application is required to be modified, but the authors argue it&amp;rsquo;s only modest
change.  Compared to other state-of-the-art (FastSwap from EuroSys&#39;20), it
achieved 13x speedup.&lt;/p&gt;
&lt;h2 id=&#34;linnos-predictability-on-unpredictable-flash-storage-with-a-light-neural-network&#34;&gt;LinnOS: Predictability on Unpredictable Flash Storage with a Light Neural Network&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=yzv9lcjxhAg&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=yzv9lcjxhAg&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s not easy to predict each I/O speed of modern fast storage devices, due to
their complex internals like the caching, the wear-leveling and the garbage
collection.  One well-known solution is hedging.  It prepare an array of SSDs,
issue I/O to one of them, and if the response doesn&amp;rsquo;t made until a timeout,
revoke the request and try with another SSD.  The wait time limit bounds the
latency.  LinnOS uses an approach similar to the hedging, but it uses a neural
network that can predict if each I/O to each SSD will be served fast or not.
For this, the neural network receives current I/O queue depth and queue depths
and latencies of last few I/Os as input.  Then, it predicts if the latency will
be only fast or slow.  To mitigate with the effect from wrong predictions, it
uses biased learning and adaptive hedging based on the prediction accuracy.
The network is learned offline.&lt;/p&gt;
&lt;h2 id=&#34;do-os-abstractions-make-sense-on-fpgas&#34;&gt;Do OS abstractions make sense on FPGAs?&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=k-cp4U3JKug&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=k-cp4U3JKug&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Computer systems embedding FPGA in addition to CPU, called heterogeneous
computing systems, are widesparead nowadays.  These systems typically have high
operational complexity.  Especially developing and deploying FPGA application
is quite complicated.  For the reason, some manufacturers provide some FPGA
shells providing some of the OS abstractions.  The authors developed a FPGA
shell called Coyote, which provides full abstraction sets of the OS and
experimented if it works well for the systems.  The result was very positive.&lt;/p&gt;
&lt;h2 id=&#34;fast-rdma-based-ordered-key-value-store-using-remote-learned-cache&#34;&gt;Fast RDMA-based Ordered Key-Value Store using Remote Learned Cache&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Qv-0YL_SII4&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=Qv-0YL_SII4&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Because server-centric key-value store (KVS) performance bound to the server
CPUs, RDMA-based approaches were proposed.  Because one RTT is required for one
RDMA, the approaches require many round trips and therefore doesn&amp;rsquo;t show high
performance.  Caching the index in client is one solution, but it makes huge
client memory footage.  The authors propose to use neural network as the index
cache.  They train neural network to receive key and provide logical address of
the value for the key in server.  The network retrained for dynamci updates and
copyied to clients.  Then, client use the nerual network, which is much smaller
than the full index tree to know the address of the value and fetch it via the
RDMA.&lt;/p&gt;
&lt;h2 id=&#34;a-simpler-and-faster-nic-driver-model-for-network-functions&#34;&gt;A Simpler and Faster NIC Driver Model for Network Functions&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=zKJIY4vbBDA&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=zKJIY4vbBDA&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Software-defined network approach, which implements functionality of appliances
like bridege, router, firewall is widespread.  For better performance and
flexibility, the software complexity has increased so that it&amp;rsquo;s hard to do
formal verification of the network stacks.  The authors argue that by
sacrificing some of the fliexibility, it&amp;rsquo;s available to implement simple and
fast network stack.  They prove their idea with a new network driver model,
tinynf.  They implemented a driver for Intel 82599 based on the driver model
with only 550 lines of code.  It was able to finish formal verification 7x
faster than a state-of-the-art driver.  Also, it achieved 1.6x performance
compared to the state-of-the-art.&lt;/p&gt;
&lt;h2 id=&#34;theseus-an-experiment-in-operating-system-structure-and-state-management&#34;&gt;Theseus: an experiment in operating system structure and state management&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=i1pLDZKtlBI&#34;&gt;https://www.youtube.com/watch?v=i1pLDZKtlBI&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this paper, the authors present an OS called Theseus that designed for state
spill problem.  For this, they made it to composed with many tiny components
called &amp;lsquo;cell&amp;rsquo;, and applied rust-like language level safe guaranteeness
mechanisms in the OS level.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
