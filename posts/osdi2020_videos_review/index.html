<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Watching OSDI 2020 presentation videos | hacklog</title>
<meta name=keywords content="osdi,osdi20,paper"><meta name=description content="I set watching at least one OSDI'20 presentation video per day during the long vacation as one of my plans.
A large scale analysis of hundreds of in-memory cache clusters at Twitter https://www.youtube.com/watch?v=OQtMM5vdhlI&amp;feature=emb_title
The authors traced and analyzed the workloads on the Twitter&rsquo;s in-memory cache systems. To me, below findings were interesting.
There are many write requests. Size of each object is not so big (median 200 bytes), so metadata, which sizes 64 bytes per each object, is a burden Size of each key is not small compared to the size of each value."><meta name=author content="Me"><link rel=canonical href=https://sjp38.github.io/posts/osdi2020_videos_review/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://sjp38.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://sjp38.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://sjp38.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://sjp38.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://sjp38.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://sjp38.github.io/posts/osdi2020_videos_review/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Watching OSDI 2020 presentation videos"><meta property="og:description" content="I set watching at least one OSDI'20 presentation video per day during the long vacation as one of my plans.
A large scale analysis of hundreds of in-memory cache clusters at Twitter https://www.youtube.com/watch?v=OQtMM5vdhlI&amp;feature=emb_title
The authors traced and analyzed the workloads on the Twitter&rsquo;s in-memory cache systems. To me, below findings were interesting.
There are many write requests. Size of each object is not so big (median 200 bytes), so metadata, which sizes 64 bytes per each object, is a burden Size of each key is not small compared to the size of each value."><meta property="og:type" content="article"><meta property="og:url" content="https://sjp38.github.io/posts/osdi2020_videos_review/"><meta property="og:image" content="https://sjp38.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-12-19T22:35:27+01:00"><meta property="article:modified_time" content="2020-12-19T22:35:27+01:00"><meta property="og:site_name" content="hacklog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://sjp38.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Watching OSDI 2020 presentation videos"><meta name=twitter:description content="I set watching at least one OSDI'20 presentation video per day during the long vacation as one of my plans.
A large scale analysis of hundreds of in-memory cache clusters at Twitter https://www.youtube.com/watch?v=OQtMM5vdhlI&amp;feature=emb_title
The authors traced and analyzed the workloads on the Twitter&rsquo;s in-memory cache systems. To me, below findings were interesting.
There are many write requests. Size of each object is not so big (median 200 bytes), so metadata, which sizes 64 bytes per each object, is a burden Size of each key is not small compared to the size of each value."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://sjp38.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Watching OSDI 2020 presentation videos","item":"https://sjp38.github.io/posts/osdi2020_videos_review/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Watching OSDI 2020 presentation videos","name":"Watching OSDI 2020 presentation videos","description":"I set watching at least one OSDI'20 presentation video per day during the long vacation as one of my plans.\nA large scale analysis of hundreds of in-memory cache clusters at Twitter https://www.youtube.com/watch?v=OQtMM5vdhlI\u0026amp;feature=emb_title\nThe authors traced and analyzed the workloads on the Twitter\u0026rsquo;s in-memory cache systems. To me, below findings were interesting.\nThere are many write requests. Size of each object is not so big (median 200 bytes), so metadata, which sizes 64 bytes per each object, is a burden Size of each key is not small compared to the size of each value.","keywords":["osdi","osdi20","paper"],"articleBody":"I set watching at least one OSDI'20 presentation video per day during the long vacation as one of my plans.\nA large scale analysis of hundreds of in-memory cache clusters at Twitter https://www.youtube.com/watch?v=OQtMM5vdhlI\u0026feature=emb_title\nThe authors traced and analyzed the workloads on the Twitter’s in-memory cache systems. To me, below findings were interesting.\nThere are many write requests. Size of each object is not so big (median 200 bytes), so metadata, which sizes 64 bytes per each object, is a burden Size of each key is not small compared to the size of each value. The trace data is available via github[1]. It’s 2.8TB for compressed version, and 14TB for uncompressed version.\n[1] https://github.com/twitter/cache-trace\nAIFM: High-Performance, Application-Integrated Far Memory https://www.youtube.com/watch?v=xHhaniGXTUg\u0026feature=emb_title\nOS-level fast-network-based far memory approaches provide transparency to the application but wastes performance due to the semantic gap due to the fixed-size page abstraction and the kernel space time consumption. AIFM solves the semantic gap using new data structure abstraction and provides user space runtime that don’t need kernel space time consumption. As a result, the application is required to be modified, but the authors argue it’s only modest change. Compared to other state-of-the-art (FastSwap from EuroSys'20), it achieved 13x speedup.\nLinnOS: Predictability on Unpredictable Flash Storage with a Light Neural Network https://www.youtube.com/watch?v=yzv9lcjxhAg\u0026feature=emb_title\nIt’s not easy to predict each I/O speed of modern fast storage devices, due to their complex internals like the caching, the wear-leveling and the garbage collection. One well-known solution is hedging. It prepare an array of SSDs, issue I/O to one of them, and if the response doesn’t made until a timeout, revoke the request and try with another SSD. The wait time limit bounds the latency. LinnOS uses an approach similar to the hedging, but it uses a neural network that can predict if each I/O to each SSD will be served fast or not. For this, the neural network receives current I/O queue depth and queue depths and latencies of last few I/Os as input. Then, it predicts if the latency will be only fast or slow. To mitigate with the effect from wrong predictions, it uses biased learning and adaptive hedging based on the prediction accuracy. The network is learned offline.\nDo OS abstractions make sense on FPGAs? https://www.youtube.com/watch?v=k-cp4U3JKug\u0026feature=emb_title\nComputer systems embedding FPGA in addition to CPU, called heterogeneous computing systems, are widesparead nowadays. These systems typically have high operational complexity. Especially developing and deploying FPGA application is quite complicated. For the reason, some manufacturers provide some FPGA shells providing some of the OS abstractions. The authors developed a FPGA shell called Coyote, which provides full abstraction sets of the OS and experimented if it works well for the systems. The result was very positive.\nFast RDMA-based Ordered Key-Value Store using Remote Learned Cache https://www.youtube.com/watch?v=Qv-0YL_SII4\u0026feature=emb_title\nBecause server-centric key-value store (KVS) performance bound to the server CPUs, RDMA-based approaches were proposed. Because one RTT is required for one RDMA, the approaches require many round trips and therefore doesn’t show high performance. Caching the index in client is one solution, but it makes huge client memory footage. The authors propose to use neural network as the index cache. They train neural network to receive key and provide logical address of the value for the key in server. The network retrained for dynamci updates and copyied to clients. Then, client use the nerual network, which is much smaller than the full index tree to know the address of the value and fetch it via the RDMA.\nA Simpler and Faster NIC Driver Model for Network Functions https://www.youtube.com/watch?v=zKJIY4vbBDA\u0026feature=emb_title\nSoftware-defined network approach, which implements functionality of appliances like bridege, router, firewall is widespread. For better performance and flexibility, the software complexity has increased so that it’s hard to do formal verification of the network stacks. The authors argue that by sacrificing some of the fliexibility, it’s available to implement simple and fast network stack. They prove their idea with a new network driver model, tinynf. They implemented a driver for Intel 82599 based on the driver model with only 550 lines of code. It was able to finish formal verification 7x faster than a state-of-the-art driver. Also, it achieved 1.6x performance compared to the state-of-the-art.\nTheseus: an experiment in operating system structure and state management https://www.youtube.com/watch?v=i1pLDZKtlBI\nIn this paper, the authors present an OS called Theseus that designed for state spill problem. For this, they made it to composed with many tiny components called ‘cell’, and applied rust-like language level safe guaranteeness mechanisms in the OS level.\nSpecification and verification in the field: Applying formal methods to BPF just-in-time compilers in the Linux kernel https://www.youtube.com/watch?v=2V3ts5-W_9g\u0026feature=emb_title\nBPF program is verified before run, but it runs after compiled via JIT, after the verification. Therefore, if there is a bug in the JIT, real problem occurs. The authors of this paper devloped a new BPF JIT for RISC-V with a JIT correctness specification framework, jitterbug. The changes made to the kernel are merged into the mainline kernel.\nStorage Systems are Distributed Systems (So Verify Them That Way!) https://www.youtube.com/watch?v=4s8EeXcu_8Y\u0026feature=emb_title\nStorage systems have high complexity, so it’s hard to do the verification. The authors of this paper realized the architecture and charactersitics of the storage systems are similar to those of distributed systems, and applied the verification methodologies for distributed systems to the storage systems after making it more general. Based on this, they implemented a verifiable key-value storage, VeriSafeKV. The performance of it was similar to unverified DB systems.\nCaladan: Mitigating Interference at Microsecond Timescales https://www.youtube.com/watch?v=G-v3ndwixOI\u0026feature=emb_title\nOn a system, multiple workloads that has different requirements co-scheduled. For example, some workloads only need best-effort resources while others require strict tail latency. Because the workloads share some hardware resources like LLC and memory bandwidth, it’s hard to fulfill all the requirements. Well know solution is resource partitioning. Because static partitioning could result in low resource utilization, dynamic partitioning solutions were previously proposed. However, the dynamic partitioning doesn’t provide microsecond granularity decision, while 100 microseconds is the marginal timewindow to guarantee the tail latency problem, according to the authors’ arguments. The authors propose to use different interference signals and design the system to work in micro-second level.\nSemeru: A Memory-Disaggregated Managed Runtime https://www.youtube.com/watch?v=MFA3MmNDKaM\u0026feature=emb_title\nConstructing system as distributed machines for different works such as CPU works and memory works are know ans resource-disaggregated architecture and gaining popularity. However, the architecture is usually written for native applications rather than GC-based applications. This paper introduces JVM runtime, Semeru, which is designed for resource-disaggregation architecture.\nPANIC: A High-Performance Programmable NIC for Multi-tenant Networks https://www.youtube.com/watch?v=EB6dK3L8Jzg\u0026feature=emb_title\nMulti-tenant networks require generality, flexible chaining, isolation, and performance to programmable NIC. However, current programmable NICs doesn’t support those. Authors propose a new programmable NIC design for the requirements and introduce an implementation on FPGA, PANIC.\nServing DNNs like Clockwork: Performance Predictability from the Bottom Up https://www.youtube.com/watch?v=wHOpY_MY57Y\u0026feature=emb_title\nNowadays, data center serves for many machine learning models for different users. Even though the latency of each inference is predictable because it has no conditional branches, the models serving system could result in unpredictable end-to-end latency. This paper proposes a new machine learning models serving system that designed do protect the latency.\nTeting Configuration Changes in Context to Prevent Production Failures https://www.youtube.com/watch?v=QrGKmp3ALKQ\u0026feature=emb_title\nIn this paper, a new test framework called ‘ctest’ is proposed for validation of configuration changes that can made in production environment. It’s similar to other test frameworks like Junit in its form.\n","wordCount":"1227","inLanguage":"en","image":"https://sjp38.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2020-12-19T22:35:27+01:00","dateModified":"2020-12-19T22:35:27+01:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://sjp38.github.io/posts/osdi2020_videos_review/"},"publisher":{"@type":"Organization","name":"hacklog","logo":{"@type":"ImageObject","url":"https://sjp38.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://sjp38.github.io/ accesskey=h title="hacklog (Alt + H)"><img src=https://sjp38.github.io/apple-touch-icon.png alt aria-label=logo height=35>hacklog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://sjp38.github.io/posts/ title=posts><span>posts</span></a></li><li><a href=https://sjp38.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://sjp38.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://sjp38.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://sjp38.github.io/files/resume_sjpark.pdf title=cv><span>cv</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://sjp38.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://sjp38.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Watching OSDI 2020 presentation videos</h1><div class=post-meta><span title='2020-12-19 22:35:27 +0100 +0100'>December 19, 2020</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1227 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/sjp38/sjp38.github.io/tree/master/src/content/posts/osdi2020_videos_review.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#a-large-scale-analysis-of-hundreds-of-in-memory-cache-clusters-at-twitter>A large scale analysis of hundreds of in-memory cache clusters at Twitter</a></li><li><a href=#aifm-high-performance-application-integrated-far-memory>AIFM: High-Performance, Application-Integrated Far Memory</a></li><li><a href=#linnos-predictability-on-unpredictable-flash-storage-with-a-light-neural-network>LinnOS: Predictability on Unpredictable Flash Storage with a Light Neural Network</a></li><li><a href=#do-os-abstractions-make-sense-on-fpgas>Do OS abstractions make sense on FPGAs?</a></li><li><a href=#fast-rdma-based-ordered-key-value-store-using-remote-learned-cache>Fast RDMA-based Ordered Key-Value Store using Remote Learned Cache</a></li><li><a href=#a-simpler-and-faster-nic-driver-model-for-network-functions>A Simpler and Faster NIC Driver Model for Network Functions</a></li><li><a href=#theseus-an-experiment-in-operating-system-structure-and-state-management>Theseus: an experiment in operating system structure and state management</a></li><li><a href=#specification-and-verification-in-the-field-applying-formal-methods-to-bpf-just-in-time-compilers-in-the-linux-kernel>Specification and verification in the field: Applying formal methods to BPF just-in-time compilers in the Linux kernel</a></li><li><a href=#storage-systems-are-distributed-systems-so-verify-them-that-way>Storage Systems are Distributed Systems (So Verify Them That Way!)</a></li><li><a href=#caladan-mitigating-interference-at-microsecond-timescales>Caladan: Mitigating Interference at Microsecond Timescales</a></li><li><a href=#semeru-a-memory-disaggregated-managed-runtime>Semeru: A Memory-Disaggregated Managed Runtime</a></li><li><a href=#panic-a-high-performance-programmable-nic-for-multi-tenant-networks>PANIC: A High-Performance Programmable NIC for Multi-tenant Networks</a></li><li><a href=#serving-dnns-like-clockwork-performance-predictability-from-the-bottom-up>Serving DNNs like Clockwork: Performance Predictability from the Bottom Up</a></li><li><a href=#teting-configuration-changes-in-context-to-prevent-production-failures>Teting Configuration Changes in Context to Prevent Production Failures</a></li></ul></nav></div></details></div><div class=post-content><p>I set watching at least one OSDI'20 presentation video per day during the
long vacation as one of my plans.</p><hr><h2 id=a-large-scale-analysis-of-hundreds-of-in-memory-cache-clusters-at-twitter>A large scale analysis of hundreds of in-memory cache clusters at Twitter<a hidden class=anchor aria-hidden=true href=#a-large-scale-analysis-of-hundreds-of-in-memory-cache-clusters-at-twitter>#</a></h2><p><a href="https://www.youtube.com/watch?v=OQtMM5vdhlI&amp;feature=emb_title">https://www.youtube.com/watch?v=OQtMM5vdhlI&amp;feature=emb_title</a></p><p>The authors traced and analyzed the workloads on the Twitter&rsquo;s in-memory cache
systems. To me, below findings were interesting.</p><ul><li>There are many write requests.</li><li>Size of each object is not so big (median 200 bytes), so metadata, which
sizes 64 bytes per each object, is a burden</li><li>Size of each key is not small compared to the size of each value.</li></ul><p>The trace data is available via github[1]. It&rsquo;s 2.8TB for compressed version,
and 14TB for uncompressed version.</p><p>[1] <a href=https://github.com/twitter/cache-trace>https://github.com/twitter/cache-trace</a></p><h2 id=aifm-high-performance-application-integrated-far-memory>AIFM: High-Performance, Application-Integrated Far Memory<a hidden class=anchor aria-hidden=true href=#aifm-high-performance-application-integrated-far-memory>#</a></h2><p><a href="https://www.youtube.com/watch?v=xHhaniGXTUg&amp;feature=emb_title">https://www.youtube.com/watch?v=xHhaniGXTUg&amp;feature=emb_title</a></p><p>OS-level fast-network-based far memory approaches provide transparency to the
application but wastes performance due to the semantic gap due to the
fixed-size page abstraction and the kernel space time consumption. AIFM solves
the semantic gap using new data structure abstraction and provides user space
runtime that don&rsquo;t need kernel space time consumption. As a result, the
application is required to be modified, but the authors argue it&rsquo;s only modest
change. Compared to other state-of-the-art (FastSwap from EuroSys'20), it
achieved 13x speedup.</p><h2 id=linnos-predictability-on-unpredictable-flash-storage-with-a-light-neural-network>LinnOS: Predictability on Unpredictable Flash Storage with a Light Neural Network<a hidden class=anchor aria-hidden=true href=#linnos-predictability-on-unpredictable-flash-storage-with-a-light-neural-network>#</a></h2><p><a href="https://www.youtube.com/watch?v=yzv9lcjxhAg&amp;feature=emb_title">https://www.youtube.com/watch?v=yzv9lcjxhAg&amp;feature=emb_title</a></p><p>It&rsquo;s not easy to predict each I/O speed of modern fast storage devices, due to
their complex internals like the caching, the wear-leveling and the garbage
collection. One well-known solution is hedging. It prepare an array of SSDs,
issue I/O to one of them, and if the response doesn&rsquo;t made until a timeout,
revoke the request and try with another SSD. The wait time limit bounds the
latency. LinnOS uses an approach similar to the hedging, but it uses a neural
network that can predict if each I/O to each SSD will be served fast or not.
For this, the neural network receives current I/O queue depth and queue depths
and latencies of last few I/Os as input. Then, it predicts if the latency will
be only fast or slow. To mitigate with the effect from wrong predictions, it
uses biased learning and adaptive hedging based on the prediction accuracy.
The network is learned offline.</p><h2 id=do-os-abstractions-make-sense-on-fpgas>Do OS abstractions make sense on FPGAs?<a hidden class=anchor aria-hidden=true href=#do-os-abstractions-make-sense-on-fpgas>#</a></h2><p><a href="https://www.youtube.com/watch?v=k-cp4U3JKug&amp;feature=emb_title">https://www.youtube.com/watch?v=k-cp4U3JKug&amp;feature=emb_title</a></p><p>Computer systems embedding FPGA in addition to CPU, called heterogeneous
computing systems, are widesparead nowadays. These systems typically have high
operational complexity. Especially developing and deploying FPGA application
is quite complicated. For the reason, some manufacturers provide some FPGA
shells providing some of the OS abstractions. The authors developed a FPGA
shell called Coyote, which provides full abstraction sets of the OS and
experimented if it works well for the systems. The result was very positive.</p><h2 id=fast-rdma-based-ordered-key-value-store-using-remote-learned-cache>Fast RDMA-based Ordered Key-Value Store using Remote Learned Cache<a hidden class=anchor aria-hidden=true href=#fast-rdma-based-ordered-key-value-store-using-remote-learned-cache>#</a></h2><p><a href="https://www.youtube.com/watch?v=Qv-0YL_SII4&amp;feature=emb_title">https://www.youtube.com/watch?v=Qv-0YL_SII4&amp;feature=emb_title</a></p><p>Because server-centric key-value store (KVS) performance bound to the server
CPUs, RDMA-based approaches were proposed. Because one RTT is required for one
RDMA, the approaches require many round trips and therefore doesn&rsquo;t show high
performance. Caching the index in client is one solution, but it makes huge
client memory footage. The authors propose to use neural network as the index
cache. They train neural network to receive key and provide logical address of
the value for the key in server. The network retrained for dynamci updates and
copyied to clients. Then, client use the nerual network, which is much smaller
than the full index tree to know the address of the value and fetch it via the
RDMA.</p><h2 id=a-simpler-and-faster-nic-driver-model-for-network-functions>A Simpler and Faster NIC Driver Model for Network Functions<a hidden class=anchor aria-hidden=true href=#a-simpler-and-faster-nic-driver-model-for-network-functions>#</a></h2><p><a href="https://www.youtube.com/watch?v=zKJIY4vbBDA&amp;feature=emb_title">https://www.youtube.com/watch?v=zKJIY4vbBDA&amp;feature=emb_title</a></p><p>Software-defined network approach, which implements functionality of appliances
like bridege, router, firewall is widespread. For better performance and
flexibility, the software complexity has increased so that it&rsquo;s hard to do
formal verification of the network stacks. The authors argue that by
sacrificing some of the fliexibility, it&rsquo;s available to implement simple and
fast network stack. They prove their idea with a new network driver model,
tinynf. They implemented a driver for Intel 82599 based on the driver model
with only 550 lines of code. It was able to finish formal verification 7x
faster than a state-of-the-art driver. Also, it achieved 1.6x performance
compared to the state-of-the-art.</p><h2 id=theseus-an-experiment-in-operating-system-structure-and-state-management>Theseus: an experiment in operating system structure and state management<a hidden class=anchor aria-hidden=true href=#theseus-an-experiment-in-operating-system-structure-and-state-management>#</a></h2><p><a href="https://www.youtube.com/watch?v=i1pLDZKtlBI">https://www.youtube.com/watch?v=i1pLDZKtlBI</a></p><p>In this paper, the authors present an OS called Theseus that designed for state
spill problem. For this, they made it to composed with many tiny components
called &lsquo;cell&rsquo;, and applied rust-like language level safe guaranteeness
mechanisms in the OS level.</p><h2 id=specification-and-verification-in-the-field-applying-formal-methods-to-bpf-just-in-time-compilers-in-the-linux-kernel>Specification and verification in the field: Applying formal methods to BPF just-in-time compilers in the Linux kernel<a hidden class=anchor aria-hidden=true href=#specification-and-verification-in-the-field-applying-formal-methods-to-bpf-just-in-time-compilers-in-the-linux-kernel>#</a></h2><p><a href="https://www.youtube.com/watch?v=2V3ts5-W_9g&amp;feature=emb_title">https://www.youtube.com/watch?v=2V3ts5-W_9g&amp;feature=emb_title</a></p><p>BPF program is verified before run, but it runs after compiled via JIT, after
the verification. Therefore, if there is a bug in the JIT, real problem
occurs. The authors of this paper devloped a new BPF JIT for RISC-V with a JIT
correctness specification framework, jitterbug. The changes made to the kernel
are merged into the mainline kernel.</p><h2 id=storage-systems-are-distributed-systems-so-verify-them-that-way>Storage Systems are Distributed Systems (So Verify Them That Way!)<a hidden class=anchor aria-hidden=true href=#storage-systems-are-distributed-systems-so-verify-them-that-way>#</a></h2><p><a href="https://www.youtube.com/watch?v=4s8EeXcu_8Y&amp;feature=emb_title">https://www.youtube.com/watch?v=4s8EeXcu_8Y&amp;feature=emb_title</a></p><p>Storage systems have high complexity, so it&rsquo;s hard to do the verification. The
authors of this paper realized the architecture and charactersitics of the
storage systems are similar to those of distributed systems, and applied the
verification methodologies for distributed systems to the storage systems after
making it more general. Based on this, they implemented a verifiable key-value
storage, VeriSafeKV. The performance of it was similar to unverified DB
systems.</p><h2 id=caladan-mitigating-interference-at-microsecond-timescales>Caladan: Mitigating Interference at Microsecond Timescales<a hidden class=anchor aria-hidden=true href=#caladan-mitigating-interference-at-microsecond-timescales>#</a></h2><p><a href="https://www.youtube.com/watch?v=G-v3ndwixOI&amp;feature=emb_title">https://www.youtube.com/watch?v=G-v3ndwixOI&amp;feature=emb_title</a></p><p>On a system, multiple workloads that has different requirements co-scheduled.
For example, some workloads only need best-effort resources while others
require strict tail latency. Because the workloads share some hardware
resources like LLC and memory bandwidth, it&rsquo;s hard to fulfill all the
requirements. Well know solution is resource partitioning. Because static
partitioning could result in low resource utilization, dynamic partitioning
solutions were previously proposed. However, the dynamic partitioning doesn&rsquo;t
provide microsecond granularity decision, while 100 microseconds is the
marginal timewindow to guarantee the tail latency problem, according to the
authors&rsquo; arguments. The authors propose to use different interference signals
and design the system to work in micro-second level.</p><h2 id=semeru-a-memory-disaggregated-managed-runtime>Semeru: A Memory-Disaggregated Managed Runtime<a hidden class=anchor aria-hidden=true href=#semeru-a-memory-disaggregated-managed-runtime>#</a></h2><p><a href="https://www.youtube.com/watch?v=MFA3MmNDKaM&amp;feature=emb_title">https://www.youtube.com/watch?v=MFA3MmNDKaM&amp;feature=emb_title</a></p><p>Constructing system as distributed machines for different works such as CPU
works and memory works are know ans resource-disaggregated architecture and
gaining popularity. However, the architecture is usually written for native
applications rather than GC-based applications. This paper introduces JVM
runtime, Semeru, which is designed for resource-disaggregation architecture.</p><h2 id=panic-a-high-performance-programmable-nic-for-multi-tenant-networks>PANIC: A High-Performance Programmable NIC for Multi-tenant Networks<a hidden class=anchor aria-hidden=true href=#panic-a-high-performance-programmable-nic-for-multi-tenant-networks>#</a></h2><p><a href="https://www.youtube.com/watch?v=EB6dK3L8Jzg&amp;feature=emb_title">https://www.youtube.com/watch?v=EB6dK3L8Jzg&amp;feature=emb_title</a></p><p>Multi-tenant networks require generality, flexible chaining, isolation, and
performance to programmable NIC. However, current programmable NICs doesn&rsquo;t
support those. Authors propose a new programmable NIC design for the
requirements and introduce an implementation on FPGA, PANIC.</p><h2 id=serving-dnns-like-clockwork-performance-predictability-from-the-bottom-up>Serving DNNs like Clockwork: Performance Predictability from the Bottom Up<a hidden class=anchor aria-hidden=true href=#serving-dnns-like-clockwork-performance-predictability-from-the-bottom-up>#</a></h2><p><a href="https://www.youtube.com/watch?v=wHOpY_MY57Y&amp;feature=emb_title">https://www.youtube.com/watch?v=wHOpY_MY57Y&amp;feature=emb_title</a></p><p>Nowadays, data center serves for many machine learning models for different
users. Even though the latency of each inference is predictable because it has
no conditional branches, the models serving system could result in
unpredictable end-to-end latency. This paper proposes a new machine learning
models serving system that designed do protect the latency.</p><h2 id=teting-configuration-changes-in-context-to-prevent-production-failures>Teting Configuration Changes in Context to Prevent Production Failures<a hidden class=anchor aria-hidden=true href=#teting-configuration-changes-in-context-to-prevent-production-failures>#</a></h2><p><a href="https://www.youtube.com/watch?v=QrGKmp3ALKQ&amp;feature=emb_title">https://www.youtube.com/watch?v=QrGKmp3ALKQ&amp;feature=emb_title</a></p><p>In this paper, a new test framework called &lsquo;ctest&rsquo; is proposed for validation
of configuration changes that can made in production environment. It&rsquo;s similar
to other test frameworks like Junit in its form.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://sjp38.github.io/tags/osdi/>Osdi</a></li><li><a href=https://sjp38.github.io/tags/osdi20/>Osdi20</a></li><li><a href=https://sjp38.github.io/tags/paper/>Paper</a></li></ul><nav class=paginav><a class=prev href=https://sjp38.github.io/posts/ko/osdi2020_videos_review/><span class=title>« Prev</span><br><span>OSDI 2020 발표 영상 감상</span>
</a><a class=next href=https://sjp38.github.io/posts/ko/remarkable2_review/><span class=title>Next »</span><br><span>Remarkable 2 리뷰</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Watching OSDI 2020 presentation videos on x" href="https://x.com/intent/tweet/?text=Watching%20OSDI%202020%20presentation%20videos&amp;url=https%3a%2f%2fsjp38.github.io%2fposts%2fosdi2020_videos_review%2f&amp;hashtags=osdi%2cosdi20%2cpaper"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Watching OSDI 2020 presentation videos on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsjp38.github.io%2fposts%2fosdi2020_videos_review%2f&amp;title=Watching%20OSDI%202020%20presentation%20videos&amp;summary=Watching%20OSDI%202020%20presentation%20videos&amp;source=https%3a%2f%2fsjp38.github.io%2fposts%2fosdi2020_videos_review%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Watching OSDI 2020 presentation videos on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsjp38.github.io%2fposts%2fosdi2020_videos_review%2f&title=Watching%20OSDI%202020%20presentation%20videos"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Watching OSDI 2020 presentation videos on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsjp38.github.io%2fposts%2fosdi2020_videos_review%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Watching OSDI 2020 presentation videos on whatsapp" href="https://api.whatsapp.com/send?text=Watching%20OSDI%202020%20presentation%20videos%20-%20https%3a%2f%2fsjp38.github.io%2fposts%2fosdi2020_videos_review%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Watching OSDI 2020 presentation videos on telegram" href="https://telegram.me/share/url?text=Watching%20OSDI%202020%20presentation%20videos&amp;url=https%3a%2f%2fsjp38.github.io%2fposts%2fosdi2020_videos_review%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Watching OSDI 2020 presentation videos on ycombinator" href="https://news.ycombinator.com/submitlink?t=Watching%20OSDI%202020%20presentation%20videos&u=https%3a%2f%2fsjp38.github.io%2fposts%2fosdi2020_videos_review%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://sjp38.github.io/>hacklog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>