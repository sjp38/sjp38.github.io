<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>researches review | hacklog</title>
    <link>/categories/researches-review/</link>
      <atom:link href="/categories/researches-review/index.xml" rel="self" type="application/rss+xml" />
    <description>researches review</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 19 Dec 2020 22:35:27 +0100</lastBuildDate>
    <image>
      <url>/img/tux.png</url>
      <title>researches review</title>
      <link>/categories/researches-review/</link>
    </image>
    
    <item>
      <title>Watching OSDI 2020 presentation videos</title>
      <link>/post/osdi2020_videos_review/</link>
      <pubDate>Sat, 19 Dec 2020 22:35:27 +0100</pubDate>
      <guid>/post/osdi2020_videos_review/</guid>
      <description>&lt;p&gt;I set watching at least one OSDI&#39;20 presentation video per day during the
long vacation as one of my plans.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;a-large-scale-analysis-of-hundreds-of-in-memory-cache-clusters-at-twitter&#34;&gt;A large scale analysis of hundreds of in-memory cache clusters at Twitter&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=OQtMM5vdhlI&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=OQtMM5vdhlI&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The authors traced and analyzed the workloads on the Twitter&amp;rsquo;s in-memory cache
systems.  To me, below findings were interesting.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are many write requests.&lt;/li&gt;
&lt;li&gt;Size of each object is not so big (median 200 bytes), so metadata, which
sizes 64 bytes per each object, is a burden&lt;/li&gt;
&lt;li&gt;Size of each key is not small compared to the size of each value.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The trace data is available via github[1].  It&amp;rsquo;s 2.8TB for compressed version,
and 14TB for uncompressed version.&lt;/p&gt;
&lt;p&gt;[1] &lt;a href=&#34;https://github.com/twitter/cache-trace&#34;&gt;https://github.com/twitter/cache-trace&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;aifm-high-performance-application-integrated-far-memory&#34;&gt;AIFM: High-Performance, Application-Integrated Far Memory&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=xHhaniGXTUg&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=xHhaniGXTUg&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;OS-level fast-network-based far memory approaches provide transparency to the
application but wastes performance due to the semantic gap due to the
fixed-size page abstraction and the kernel space time consumption.  AIFM solves
the semantic gap using new data structure abstraction and provides user space
runtime that don&amp;rsquo;t need kernel space time consumption.  As a result, the
application is required to be modified, but the authors argue it&amp;rsquo;s only modest
change.  Compared to other state-of-the-art (FastSwap from EuroSys&#39;20), it
achieved 13x speedup.&lt;/p&gt;
&lt;h2 id=&#34;linnos-predictability-on-unpredictable-flash-storage-with-a-light-neural-network&#34;&gt;LinnOS: Predictability on Unpredictable Flash Storage with a Light Neural Network&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=yzv9lcjxhAg&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=yzv9lcjxhAg&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s not easy to predict each I/O speed of modern fast storage devices, due to
their complex internals like the caching, the wear-leveling and the garbage
collection.  One well-known solution is hedging.  It prepare an array of SSDs,
issue I/O to one of them, and if the response doesn&amp;rsquo;t made until a timeout,
revoke the request and try with another SSD.  The wait time limit bounds the
latency.  LinnOS uses an approach similar to the hedging, but it uses a neural
network that can predict if each I/O to each SSD will be served fast or not.
For this, the neural network receives current I/O queue depth and queue depths
and latencies of last few I/Os as input.  Then, it predicts if the latency will
be only fast or slow.  To mitigate with the effect from wrong predictions, it
uses biased learning and adaptive hedging based on the prediction accuracy.
The network is learned offline.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
