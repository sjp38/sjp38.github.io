<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Researches Review on hacklog</title>
    <link>https://sjp38.github.io/categories/researches-review/</link>
    <description>Recent content in Researches Review on hacklog</description>
    <image>
      <title>hacklog</title>
      <url>https://sjp38.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://sjp38.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.131.0</generator>
    <language>en</language>
    <lastBuildDate>Sat, 07 Aug 2021 09:27:55 +0200</lastBuildDate>
    <atom:link href="https://sjp38.github.io/categories/researches-review/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>OSDI 2021 papers summary</title>
      <link>https://sjp38.github.io/posts/en/osdi2021_papers_review/</link>
      <pubDate>Sat, 07 Aug 2021 09:27:55 +0200</pubDate>
      <guid>https://sjp38.github.io/posts/en/osdi2021_papers_review/</guid>
      <description>This post is for recording some notes from a few OSDI&#39;21 papers that I got fun.
DMon: Efficient Detection and Correction of Data Locality Problems Using Selective Profiling https://www.usenix.org/system/files/osdi21-khan.pdf
DMon is a compiler-based data locality optimization system. The approach is quite similar to that of daphicx or similar things. It injects data access profiling code in the target program, build and run it with some workload, collect the profiling code-generated results, analyze the profile results to figure out what kind of optimization can provide some benefit to the program, injects the optimization code in the program, built it again, and deploy the version to the production.</description>
    </item>
    <item>
      <title>OSDI 2020 발표 영상 감상</title>
      <link>https://sjp38.github.io/posts/ko/osdi2020_videos_review/</link>
      <pubDate>Sat, 19 Dec 2020 22:35:27 +0100</pubDate>
      <guid>https://sjp38.github.io/posts/ko/osdi2020_videos_review/</guid>
      <description>차일 피일 미루고 있던 OSDI&#39;20 발표 영상 비디오를 휴가 기간동안 하루 한편이라도 보기로 했습니다.
A large scale analysis of hundreds of in-memory cache clusters at Twitter https://www.youtube.com/watch?v=OQtMM5vdhlI&amp;amp;feature=emb_title
트위터의 in-memory 캐시 시스템의 워크로드를 트레이스하고 그 특성을 분석한 논문입니다. 개인적으로 아래 내용이 흥미로웠습니다.
쓰기 리퀘스트가 많음. 각 오브젝트의 크기는 작아서 (중간값이 200 바이트), 오브젝트별 메타데이터 (64 바이트) 가 공간을 많이 차지함. 키의 크기가 밸류의 크기보다 그렇게 작지 않음. 트레이스 데이터는 github[1] 통해 받을 수 있습니다.</description>
    </item>
    <item>
      <title>Watching OSDI 2020 presentation videos</title>
      <link>https://sjp38.github.io/posts/en/osdi2020_videos_review/</link>
      <pubDate>Sat, 19 Dec 2020 22:35:27 +0100</pubDate>
      <guid>https://sjp38.github.io/posts/en/osdi2020_videos_review/</guid>
      <description>I set watching at least one OSDI&#39;20 presentation video per day during the long vacation as one of my plans.
A large scale analysis of hundreds of in-memory cache clusters at Twitter https://www.youtube.com/watch?v=OQtMM5vdhlI&amp;amp;feature=emb_title
The authors traced and analyzed the workloads on the Twitter&amp;rsquo;s in-memory cache systems. To me, below findings were interesting.
There are many write requests. Size of each object is not so big (median 200 bytes), so metadata, which sizes 64 bytes per each object, is a burden Size of each key is not small compared to the size of each value.</description>
    </item>
  </channel>
</rss>
