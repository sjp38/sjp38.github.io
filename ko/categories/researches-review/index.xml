<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>researches review | hacklog</title>
    <link>/ko/categories/researches-review/</link>
      <atom:link href="/ko/categories/researches-review/index.xml" rel="self" type="application/rss+xml" />
    <description>researches review</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>ko-kr</language><lastBuildDate>Sat, 19 Dec 2020 22:35:27 +0100</lastBuildDate>
    <image>
      <url>/img/tux.png</url>
      <title>researches review</title>
      <link>/ko/categories/researches-review/</link>
    </image>
    
    <item>
      <title>OSDI 2020 발표 영상 감상</title>
      <link>/ko/post/osdi2020_videos_review/</link>
      <pubDate>Sat, 19 Dec 2020 22:35:27 +0100</pubDate>
      <guid>/ko/post/osdi2020_videos_review/</guid>
      <description>&lt;p&gt;차일 피일 미루고 있던 OSDI&#39;20 발표 영상 비디오를 휴가 기간동안 하루 한편이라도
보기로 했습니다.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;a-large-scale-analysis-of-hundreds-of-in-memory-cache-clusters-at-twitter&#34;&gt;A large scale analysis of hundreds of in-memory cache clusters at Twitter&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=OQtMM5vdhlI&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=OQtMM5vdhlI&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;트위터의 in-memory 캐시 시스템의 워크로드를 트레이스하고 그 특성을 분석한
논문입니다.  개인적으로 아래 내용이 흥미로웠습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;쓰기 리퀘스트가 많음.&lt;/li&gt;
&lt;li&gt;각 오브젝트의 크기는 작아서 (중간값이 200 바이트), 오브젝트별 메타데이터 (64
바이트) 가 공간을 많이 차지함.&lt;/li&gt;
&lt;li&gt;키의 크기가 밸류의 크기보다 그렇게 작지 않음.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;트레이스 데이터는 github[1] 통해 받을 수 있습니다.  압축해도 2.8TB, 압출 풀면
14TB 군요.&lt;/p&gt;
&lt;p&gt;[1] &lt;a href=&#34;https://github.com/twitter/cache-trace&#34;&gt;https://github.com/twitter/cache-trace&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;aifm-high-performance-application-integrated-far-memory&#34;&gt;AIFM: High-Performance, Application-Integrated Far Memory&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=xHhaniGXTUg&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=xHhaniGXTUg&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;OS 수준에서의 fast network 기반 far memory 접근법은 어플리케이션의 수정이
필요없다는 장점을 갖지만 고정된 크기의 페이지 abstraction 에 따른 semantic
차이와 어플리케이션에 대한 지식이 없는 커널에서의 주요 오퍼레이션 수행으로 인해
성능이 떨어집니다.  AIFM 은 새로운 데이터 구조 abstraction 을 사용해 semantic
차이를 해결하고 user space 런타임 시스템을 사용해 kernel space 에서의 시간
낭비를 제거했습니다.  결과적으로 어플리케이션은 수정이 필요하지만 저자들은
약간의 수정일 뿐이라 주자합니다.  또다른 state-of-the-art (FastSwap,
EuroSys&#39;20) 대비 13배 성능 향상을 이뤘다는군요.&lt;/p&gt;
&lt;h2 id=&#34;linnos-predictability-on-unpredictable-flash-storage-with-a-light-neural-network&#34;&gt;LinnOS: Predictability on Unpredictable Flash Storage with a Light Neural Network&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=yzv9lcjxhAg&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=yzv9lcjxhAg&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;최신 고속 저장장치는 캐싱, 웨어레벨링, 가비지 콜렉션 등의 복잡한 내부 구조를
가지고 있어서, 각 I/O 에 대한 응답시간을 예측하기가 어렵습니다.  이에 대한 잘
알려진 해결책은 Hedging 입니다.  여러 SSD 를 준비해 두고, I/O 요청을 그 중
하나의 SSD 에 일단 던지고, 그 응답이 미리 지정된 한계치를 넘도록 돌아오지
않으면 해당 요청을 취소하고 다른 SSD 를 쓰는 것이죠.  한계치 만큼은 기다려야
한다는 게 약점입니다.  LinnOS 는 Hedging 과 유사하지만 각 SSD 로의 각 I/O 가
빠르게 처리될지 예측하는 신경망을 이용합니다.  이를 위해, 해당 신경망은 현재
I/O queue depth, 최근의 몇개 I/O 시 queue depth 와 latency 를 입력으로 받고 그
결과 레이턴시가 빠를지 느릴지만 예측합니다.  잘못된 예측으로 인한 문제를
처리하기 위해선 biased learning 과 예측 정확도에 따른 adaptive hedging 을
사용합니다.  신경망의 학습은 오프라인으로 이루어집니다.&lt;/p&gt;
&lt;h2 id=&#34;do-os-abstractions-make-sense-on-fpgas&#34;&gt;Do OS abstractions make sense on FPGAs?&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=k-cp4U3JKug&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=k-cp4U3JKug&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Heterogeneous computing system 이라고도 불리는, FPGA 를 내장한 컴퓨터 시스템이
널리 사용되고 있습니다.  이런 시스템에는 운영 복잡성이 높게 마련이며, 특히 FPGA
용 어플리케이션의 개발과 배포가 복잡합니다.  때문에 FPGA 관리를 위해 운영체제의
추상화 개념이 일부 사용되고 있습니다.  저자는 Coyote 라고 하는, 운영체제의
일반적 추상화 개념을 모두 지원하는 FPGA 관리 도구를 만들어 운영체제 추상화
개념이 FPGA 에 잘 적용되는지 실험했고, 그에 대한 긍정적 결과를 얻었습니다.&lt;/p&gt;
&lt;h2 id=&#34;fast-rdma-based-ordered-key-value-store-using-remote-learned-cache&#34;&gt;Fast RDMA-based Ordered Key-Value Store using Remote Learned Cache&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Qv-0YL_SII4&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=Qv-0YL_SII4&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;서버 중심적 key-value store (KVS) 는 서버의 CPU 에 성능이 제한되므로, RDMA
기반의 방법들이 제안되어왔습니다.  한번의 RDMA 에 한번의 RTT 가 소요되므로, 이
방법은 여러번 네트워크 순회를 해야 하는 문제가 있어 성능이 떨어지는 문제가
있습니다.  인덱스를 클라이언트 내에 캐시해 두는 해결책도 있습니다만,
클라이언트의 메모리 사용량을 급격하게 늘린다는 문제가 있습니다.  이 논문의
저자들은 신경망을 인덱스로 사용하자는 제안을 합니다.  키를 입력으로 받아 밸류의
주소를 내놓는 신경망을 서버 측에서 학습시키고, 전체 인덱스에 비해 훨씬 작은 이
신경망을 클라이언트로 전송, 각 클라이언트가 이 신경망을 캐시로 사용해 밸류의
서버내 주소를 알아내고, RDMA 로 한번에 밸류를 얻어오는 방법입니다.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
