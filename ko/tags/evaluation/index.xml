<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>evaluation | hacklog</title>
    <link>https://sjp38.github.io/ko/tags/evaluation/</link>
      <atom:link href="https://sjp38.github.io/ko/tags/evaluation/index.xml" rel="self" type="application/rss+xml" />
    <description>evaluation</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>ko-kr</language><lastBuildDate>Sun, 15 Jan 2023 10:34:06 -0800</lastBuildDate>
    <image>
      <url>https://sjp38.github.io/img/tux.png</url>
      <title>evaluation</title>
      <link>https://sjp38.github.io/ko/tags/evaluation/</link>
    </image>
    
    <item>
      <title>DAMON Evaluation</title>
      <link>https://sjp38.github.io/ko/post/damon_evaluation/</link>
      <pubDate>Sun, 15 Jan 2023 10:34:06 -0800</pubDate>
      <guid>https://sjp38.github.io/ko/post/damon_evaluation/</guid>
      <description>&lt;p&gt;DAMON is lightweight.  It increases system memory usage by 0.39% and slows
target workloads down by 1.16%.&lt;/p&gt;
&lt;p&gt;DAMON is accurate and useful for memory management optimizations.  An
experimental DAMON-based operation scheme for THP, namely &amp;lsquo;ethp&amp;rsquo;, removes
76.15% of THP memory overheads while preserving 51.25% of THP speedup.  Another
experimental DAMON-based &amp;lsquo;proactive reclamation&amp;rsquo; implementation, namely &amp;lsquo;prcl&amp;rsquo;,
reduces 93.38% of residential sets and 23.63% of system memory footprint while
incurring only 1.22% runtime overhead in the best case (parsec3/freqmine).&lt;/p&gt;
&lt;h1 id=&#34;setup&#34;&gt;Setup&lt;/h1&gt;
&lt;p&gt;On QEMU/KVM based virtual machines utilizing 130GB of RAM and 36 vCPUs hosted
by AWS EC2 i3.metal instances that running a kernel that v24 DAMON patchset is
applied, I measure runtime and consumed system memory while running various
realistic workloads with several configurations.  From each of PARSEC3 [3] and
SPLASH-2X [4] benchmark suites I pick 12 workloads, so I use 24 workloads in
total.  I use another wrapper scripts [5] for convenient setup and run of the
workloads.&lt;/p&gt;
&lt;h2 id=&#34;measurement&#34;&gt;Measurement&lt;/h2&gt;
&lt;p&gt;For the measurement of the amount of consumed memory in system global scope, I
drop caches before starting each of the workloads and monitor &amp;lsquo;MemFree&amp;rsquo; in the
&amp;lsquo;/proc/meminfo&amp;rsquo; file.  To make results more stable, I repeat the runs 5 times
and average results.&lt;/p&gt;
&lt;h2 id=&#34;configurations&#34;&gt;Configurations&lt;/h2&gt;
&lt;p&gt;The configurations I use are as below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;orig: Linux v5.10 with &amp;lsquo;madvise&amp;rsquo; THP policy&lt;/li&gt;
&lt;li&gt;rec: &amp;lsquo;orig&amp;rsquo; plus DAMON running with virtual memory access recording&lt;/li&gt;
&lt;li&gt;prec: &amp;lsquo;orig&amp;rsquo; plus DAMON running with physical memory access recording&lt;/li&gt;
&lt;li&gt;thp: same with &amp;lsquo;orig&amp;rsquo;, but use &amp;lsquo;always&amp;rsquo; THP policy&lt;/li&gt;
&lt;li&gt;ethp: &amp;lsquo;orig&amp;rsquo; plus a DAMON operation scheme, &amp;lsquo;efficient THP&amp;rsquo;&lt;/li&gt;
&lt;li&gt;prcl: &amp;lsquo;orig&amp;rsquo; plus a DAMON operation scheme, &amp;lsquo;proactive reclaim [6]&amp;rsquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I use &amp;lsquo;rec&amp;rsquo; for measurement of DAMON overheads to target workloads and system
memory.  &amp;lsquo;prec&amp;rsquo; is for physical memory monitroing and recording.  It monitors
17GB sized &amp;lsquo;System RAM&amp;rsquo; region.  The remaining configs including &amp;lsquo;thp&amp;rsquo;, &amp;lsquo;ethp&amp;rsquo;,
and &amp;lsquo;prcl&amp;rsquo; are for measurement of DAMON monitoring accuracy.&lt;/p&gt;
&lt;p&gt;&amp;lsquo;ethp&amp;rsquo; and &amp;lsquo;prcl&amp;rsquo; are simple DAMON-based operation schemes developed for
proof of concepts of DAMON.  &amp;lsquo;ethp&amp;rsquo; reduces memory space waste of THP [1,2],
by using DAMON for the decision of promotions and demotion for huge pages,
while &amp;lsquo;prcl&amp;rsquo; is as similar as the original work.  For example, those can be
implemented as below::&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# format: &amp;lt;min/max size&amp;gt; &amp;lt;min/max frequency (0-100)&amp;gt; &amp;lt;min/max age&amp;gt; &amp;lt;action&amp;gt;
# ethp: Use huge pages if a region shows &amp;gt;=5% access rate, use regular
# pages if a region &amp;gt;=2MB shows 0 access rate for &amp;gt;=7 seconds
min     max     5       max     min     max     hugepage
2M      max     min     min     7s      max     nohugepage

# prcl: If a region &amp;gt;=4KB shows 0 access rate for &amp;gt;=5 seconds, page out.
4K      max     0       0       5s     max     pageout
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that these examples are designed with my only straightforward intuition
because those are for only proof of concepts and monitoring accuracy of DAMON.
In other words, those are not for production.  For production use, those should
be more tuned.  For automation of such tuning, you can use a user space tool
called DAMOOS [8].  For the evaluation, we use &amp;lsquo;ethp&amp;rsquo; as same to above
example, but we use DAMOOS-tuned &amp;lsquo;prcl&amp;rsquo; for each workload.&lt;/p&gt;
&lt;p&gt;The evaluation is done using the tests package for DAMON, &lt;code&gt;damon-tests&lt;/code&gt; [7].
Using it, you can do the evaluation and generate a report on your own.&lt;/p&gt;
&lt;p&gt;[1] &amp;ldquo;Redis latency problems troubleshooting&amp;rdquo;, &lt;a href=&#34;https://redis.io/topics/latency&#34;&gt;https://redis.io/topics/latency&lt;/a&gt;&lt;br&gt;
[2] &amp;ldquo;Disable Transparent Huge Pages (THP)&amp;quot;,
&lt;a href=&#34;https://docs.mongodb.com/manual/tutorial/transparent-huge-pages/&#34;&gt;https://docs.mongodb.com/manual/tutorial/transparent-huge-pages/&lt;/a&gt;&lt;br&gt;
[3] &amp;ldquo;The PARSEC Becnhmark Suite&amp;rdquo;, &lt;a href=&#34;https://parsec.cs.princeton.edu/index.htm&#34;&gt;https://parsec.cs.princeton.edu/index.htm&lt;/a&gt;&lt;br&gt;
[4] &amp;ldquo;SPLASH-2x&amp;rdquo;, &lt;a href=&#34;https://parsec.cs.princeton.edu/parsec3-doc.htm#splash2x&#34;&gt;https://parsec.cs.princeton.edu/parsec3-doc.htm#splash2x&lt;/a&gt;&lt;br&gt;
[5] &amp;ldquo;parsec3_on_ubuntu&amp;rdquo;, &lt;a href=&#34;https://github.com/sjp38/parsec3_on_ubuntu&#34;&gt;https://github.com/sjp38/parsec3_on_ubuntu&lt;/a&gt;&lt;br&gt;
[6] &amp;ldquo;Proactively reclaiming idle memory&amp;rdquo;, &lt;a href=&#34;https://lwn.net/Articles/787611/&#34;&gt;https://lwn.net/Articles/787611/&lt;/a&gt;&lt;br&gt;
[7] &amp;ldquo;damon-tests&amp;rdquo;, &lt;a href=&#34;https://github.com/awslabs/damon-tests&#34;&gt;https://github.com/awslabs/damon-tests&lt;/a&gt;&lt;br&gt;
[8] &amp;ldquo;DAMOOS&amp;rdquo;, &lt;a href=&#34;https://github.com/awslabs/damoos&#34;&gt;https://github.com/awslabs/damoos&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;results&#34;&gt;Results&lt;/h1&gt;
&lt;p&gt;Below two tables show the measurement results.  The runtimes are in seconds
while the memory usages are in KiB.  Each configuration except &amp;lsquo;orig&amp;rsquo; shows
its overhead relative to &amp;lsquo;orig&amp;rsquo; in percent within parenthesizes.::&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;runtime                 orig     rec      (overhead) prec     (overhead) thp      (overhead) ethp     (overhead) prcl     (overhead)
parsec3/blackscholes    139.658  140.168  (0.37)     139.385  (-0.20)    138.367  (-0.92)    139.279  (-0.27)    147.024  (5.27)
parsec3/bodytrack       123.788  124.622  (0.67)     123.636  (-0.12)    125.115  (1.07)     123.840  (0.04)     141.928  (14.65)
parsec3/canneal         207.491  210.318  (1.36)     217.927  (5.03)     174.287  (-16.00)   202.609  (-2.35)    225.483  (8.67)
parsec3/dedup           18.292   18.301   (0.05)     18.378   (0.47)     18.264   (-0.15)    18.298   (0.03)     20.541   (12.30)
parsec3/facesim         343.893  340.286  (-1.05)    338.217  (-1.65)    332.953  (-3.18)    333.840  (-2.92)    365.650  (6.33)
parsec3/fluidanimate    339.959  326.886  (-3.85)    330.286  (-2.85)    331.239  (-2.57)    326.011  (-4.10)    341.684  (0.51)
parsec3/freqmine        445.987  436.332  (-2.16)    435.946  (-2.25)    435.704  (-2.31)    437.595  (-1.88)    451.414  (1.22)
parsec3/raytrace        184.106  182.158  (-1.06)    182.056  (-1.11)    183.180  (-0.50)    183.545  (-0.30)    202.197  (9.83)
parsec3/streamcluster   599.990  674.091  (12.35)    617.314  (2.89)     521.864  (-13.02)   551.971  (-8.00)    696.127  (16.02)
parsec3/swaptions       220.462  222.637  (0.99)     220.449  (-0.01)    219.921  (-0.25)    221.607  (0.52)     223.956  (1.59)
parsec3/vips            87.767   88.700   (1.06)     87.461   (-0.35)    87.466   (-0.34)    87.875   (0.12)     91.768   (4.56)
parsec3/x264            110.843  117.856  (6.33)     113.023  (1.97)     108.665  (-1.97)    115.434  (4.14)     117.811  (6.29)
splash2x/barnes         131.441  129.275  (-1.65)    128.341  (-2.36)    119.317  (-9.22)    126.199  (-3.99)    147.602  (12.30)
splash2x/fft            59.705   58.382   (-2.22)    58.858   (-1.42)    45.949   (-23.04)   59.939   (0.39)     64.548   (8.11)
splash2x/lu_cb          132.552  131.604  (-0.72)    131.846  (-0.53)    132.320  (-0.18)    132.100  (-0.34)    140.289  (5.84)
splash2x/lu_ncb         150.215  149.670  (-0.36)    149.646  (-0.38)    148.823  (-0.93)    149.416  (-0.53)    152.338  (1.41)
splash2x/ocean_cp       84.033   76.405   (-9.08)    75.104   (-10.63)   73.487   (-12.55)   77.789   (-7.43)    77.380   (-7.92)
splash2x/ocean_ncp      153.833  154.247  (0.27)     156.227  (1.56)     106.619  (-30.69)   139.299  (-9.45)    165.030  (7.28)
splash2x/radiosity      143.566  143.654  (0.06)     142.426  (-0.79)    141.193  (-1.65)    141.740  (-1.27)    157.817  (9.93)
splash2x/radix          49.984   49.996   (0.02)     50.519   (1.07)     46.573   (-6.82)    50.724   (1.48)     50.695   (1.42)
splash2x/raytrace       133.238  134.337  (0.83)     134.389  (0.86)     134.833  (1.20)     131.073  (-1.62)    145.541  (9.23)
splash2x/volrend        121.700  120.652  (-0.86)    120.560  (-0.94)    120.629  (-0.88)    119.581  (-1.74)    129.422  (6.35)
splash2x/water_nsquared 370.771  375.236  (1.20)     376.829  (1.63)     355.592  (-4.09)    354.087  (-4.50)    419.606  (13.17)
splash2x/water_spatial  133.295  132.931  (-0.27)    132.762  (-0.40)    133.090  (-0.15)    133.809  (0.39)     153.647  (15.27)
total                   4486.580 4538.750 (1.16)     4481.600 (-0.11)    4235.430 (-5.60)    4357.660 (-2.87)    4829.510 (7.64)


memused.avg             orig         rec          (overhead) prec         (overhead) thp          (overhead) ethp         (overhead) prcl         (overhead)
parsec3/blackscholes    1828693.600  1834084.000  (0.29)     1823839.800  (-0.27)    1819296.600  (-0.51)    1830281.800  (0.09)     1603975.800  (-12.29)
parsec3/bodytrack       1424963.400  1440085.800  (1.06)     1438384.200  (0.94)     1421718.400  (-0.23)    1432834.600  (0.55)     1439283.000  (1.00)
parsec3/canneal         1036782.600  1052828.800  (1.55)     1050148.600  (1.29)     1035104.400  (-0.16)    1051145.400  (1.39)     1050019.400  (1.28)
parsec3/dedup           2511841.400  2507374.000  (-0.18)    2472450.600  (-1.57)    2523557.600  (0.47)     2508912.000  (-0.12)    2493347.200  (-0.74)
parsec3/facesim         537769.800   550740.800   (2.41)     548683.600   (2.03)     543547.800   (1.07)     560556.600   (4.24)     482782.600   (-10.23)
parsec3/fluidanimate    570268.600   585598.000   (2.69)     579837.800   (1.68)     571433.000   (0.20)     582112.800   (2.08)     470073.400   (-17.57)
parsec3/freqmine        982941.400   996253.200   (1.35)     993919.800   (1.12)     990531.800   (0.77)     1000994.400  (1.84)     750685.800   (-23.63)
parsec3/raytrace        1737446.000  1749908.800  (0.72)     1741183.800  (0.22)     1726674.800  (-0.62)    1748530.200  (0.64)     1552275.600  (-10.66)
parsec3/streamcluster   115857.000   155194.400   (33.95)    158272.800   (36.61)    122125.200   (5.41)     134545.600   (16.13)    133448.600   (15.18)
parsec3/swaptions       13694.200    28451.800    (107.76)   28464.600    (107.86)   12797.800    (-6.55)    25328.200    (84.96)    28138.400    (105.48)
parsec3/vips            2976126.400  3002408.600  (0.88)     3008218.800  (1.08)     2978258.600  (0.07)     2995428.600  (0.65)     2936338.600  (-1.34)
parsec3/x264            3233886.200  3258790.200  (0.77)     3248355.000  (0.45)     3232070.000  (-0.06)    3256360.200  (0.69)     3254707.400  (0.64)
splash2x/barnes         1210470.600  1211918.600  (0.12)     1204507.000  (-0.49)    1210892.800  (0.03)     1217414.800  (0.57)     944053.400   (-22.01)
splash2x/fft            9697440.000  9604535.600  (-0.96)    9210571.800  (-5.02)    9867368.000  (1.75)     9637571.800  (-0.62)    9804092.000  (1.10)
splash2x/lu_cb          510680.400   521792.600   (2.18)     517724.600   (1.38)     513500.800   (0.55)     519980.600   (1.82)     351787.000   (-31.11)
splash2x/lu_ncb         512896.200   529353.600   (3.21)     521248.600   (1.63)     513493.200   (0.12)     523793.400   (2.12)     418701.600   (-18.37)
splash2x/ocean_cp       3320800.200  3313688.400  (-0.21)    3225585.000  (-2.87)    3359032.200  (1.15)     3316591.800  (-0.13)    3304702.200  (-0.48)
splash2x/ocean_ncp      3915132.400  3917401.000  (0.06)     3884086.400  (-0.79)    7050398.600  (80.08)    4532528.600  (15.77)    3920395.800  (0.13)
splash2x/radiosity      1456908.200  1467611.800  (0.73)     1453612.600  (-0.23)    1466695.400  (0.67)     1467495.600  (0.73)     421197.600   (-71.09)
splash2x/radix          2345874.600  2318202.200  (-1.18)    2261499.200  (-3.60)    2438228.400  (3.94)     2373697.800  (1.19)     2336605.600  (-0.40)
splash2x/raytrace       43258.800    57624.200    (33.21)    55164.600    (27.52)    46204.400    (6.81)     60475.000    (39.80)    48865.400    (12.96)
splash2x/volrend        149615.000   163809.400   (9.49)     162115.400   (8.36)     149119.600   (-0.33)    162747.800   (8.78)     157734.600   (5.43)
splash2x/water_nsquared 40384.400    54848.600    (35.82)    53796.600    (33.21)    41455.800    (2.65)     53226.400    (31.80)    58260.600    (44.27)
splash2x/water_spatial  670580.200   680444.200   (1.47)     670020.400   (-0.08)    668262.400   (-0.35)    678552.000   (1.19)     372931.000   (-44.39)
total                   40844300.000 41002900.000 (0.39)     40311600.000 (-1.30)    44301900.000 (8.47)     41671052.000 (2.02)     38334431.000 (-6.14)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;damon-overheads&#34;&gt;DAMON Overheads&lt;/h2&gt;
&lt;p&gt;In total, DAMON virtual memory access recording feature (&amp;lsquo;rec&amp;rsquo;) incurs 1.16%
runtime overhead and 0.39% memory space overhead.  Even though the size of the
monitoring target region becomes much larger with the physical memory access
recording (&amp;lsquo;prec&amp;rsquo;), it still shows only modest amount of overhead (-0.11% for
runtime and -1.30% for memory footprint).&lt;/p&gt;
&lt;p&gt;For a convenient test run of &amp;lsquo;rec&amp;rsquo; and &amp;lsquo;prec&amp;rsquo;, I use a Python wrapper.  The
wrapper constantly consumes about 10-15MB of memory.  This becomes a high
memory overhead if the target workload has a small memory footprint.
Nonetheless, the overheads are not from DAMON, but from the wrapper, and thus
should be ignored.  This fake memory overhead continues in &amp;lsquo;ethp&amp;rsquo; and &amp;lsquo;prcl&amp;rsquo;,
as those configurations are also using the Python wrapper.&lt;/p&gt;
&lt;h2 id=&#34;efficient-thp&#34;&gt;Efficient THP&lt;/h2&gt;
&lt;p&gt;THP &amp;lsquo;always&amp;rsquo; enabled policy achieves 5.60% speedup but incurs 8.47% memory
overhead.  It achieves 30.69% speedup in the best case, but 80.08% memory
overhead in the worst case.  Interestingly, both the best and worst-case are
with &amp;lsquo;splash2x/ocean_ncp&amp;rsquo;).&lt;/p&gt;
&lt;p&gt;The 2-lines implementation of data access monitoring based THP version (&amp;lsquo;ethp&amp;rsquo;)
shows 2.87% speedup and 2.02% memory overhead.  In other words, &amp;lsquo;ethp&amp;rsquo; removes
76.15% of THP memory waste while preserving 51.25% of THP speedup in total.  In
the case of the &amp;lsquo;splash2x/ocean_ncp&amp;rsquo;, &amp;lsquo;ethp&amp;rsquo; removes 80.30% of THP memory waste
while preserving 30.79% of THP speedup.&lt;/p&gt;
&lt;h2 id=&#34;proactive-reclamation&#34;&gt;Proactive Reclamation&lt;/h2&gt;
&lt;p&gt;As similar to the original work, I use 4G &amp;lsquo;zram&amp;rsquo; swap device for this
configuration.  Also note that we use DAMOOS-tuned ethp schemes for each
workload.&lt;/p&gt;
&lt;p&gt;In total, our 1 line implementation of Proactive Reclamation, &amp;lsquo;prcl&amp;rsquo;, incurred
7.64% runtime overhead in total while achieving 6.14% system memory footprint
reduction.  Even in the worst case, the runtime overhead was only 16.02%.&lt;/p&gt;
&lt;p&gt;Nonetheless, as the memory usage is calculated with &amp;lsquo;MemFree&amp;rsquo; in
&amp;lsquo;/proc/meminfo&amp;rsquo;, it contains the SwapCached pages.  As the swapcached pages can
be easily evicted, I also measured the residential set size of the workloads::&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;rss.avg                 orig         rec          (overhead) prec         (overhead) thp          (overhead) ethp         (overhead) prcl         (overhead)
parsec3/blackscholes    587536.800   585720.000   (-0.31)    586233.400   (-0.22)    587045.400   (-0.08)    586753.400   (-0.13)    252207.400   (-57.07)
parsec3/bodytrack       32302.200    32290.600    (-0.04)    32261.800    (-0.13)    32215.800    (-0.27)    32173.000    (-0.40)    6798.800     (-78.95)
parsec3/canneal         842370.600   841443.400   (-0.11)    844012.400   (0.19)     838074.400   (-0.51)    841700.800   (-0.08)    840804.000   (-0.19)
parsec3/dedup           1180414.800  1164634.600  (-1.34)    1188886.200  (0.72)     1207821.000  (2.32)     1193896.200  (1.14)     572359.200   (-51.51)
parsec3/facesim         311848.400   311709.800   (-0.04)    311790.800   (-0.02)    317345.800   (1.76)     315443.400   (1.15)     188488.000   (-39.56)
parsec3/fluidanimate    531868.000   531885.600   (0.00)     531828.800   (-0.01)    532988.000   (0.21)     532959.600   (0.21)     415153.200   (-21.94)
parsec3/freqmine        552491.000   552718.600   (0.04)     552807.200   (0.06)     556574.200   (0.74)     554374.600   (0.34)     36573.400    (-93.38)
parsec3/raytrace        879683.400   880752.200   (0.12)     879907.000   (0.03)     870631.000   (-1.03)    880952.200   (0.14)     293119.200   (-66.68)
parsec3/streamcluster   110991.800   110937.200   (-0.05)    110964.600   (-0.02)    115606.800   (4.16)     116199.000   (4.69)     110108.200   (-0.80)
parsec3/swaptions       5665.000     5718.400     (0.94)     5720.600     (0.98)     5682.200     (0.30)     5628.600     (-0.64)    3613.800     (-36.21)
parsec3/vips            32143.600    31823.200    (-1.00)    31912.200    (-0.72)    33164.200    (3.18)     33925.800    (5.54)     27813.600    (-13.47)
parsec3/x264            81534.000    81811.000    (0.34)     81708.400    (0.21)     83052.400    (1.86)     83758.800    (2.73)     81691.800    (0.19)
splash2x/barnes         1220515.200  1218291.200  (-0.18)    1217699.600  (-0.23)    1228551.600  (0.66)     1220669.800  (0.01)     681096.000   (-44.20)
splash2x/fft            9915850.400  10036461.000 (1.22)     9881242.800  (-0.35)    10334603.600 (4.22)     10006993.200 (0.92)     8975181.200  (-9.49)
splash2x/lu_cb          511327.200   511679.000   (0.07)     511761.600   (0.08)     511971.600   (0.13)     511711.200   (0.08)     338005.000   (-33.90)
splash2x/lu_ncb         511505.000   506816.800   (-0.92)    511392.800   (-0.02)    496623.000   (-2.91)    511410.200   (-0.02)    404734.000   (-20.87)
splash2x/ocean_cp       3398834.000  3405017.800  (0.18)     3415287.800  (0.48)     3443604.600  (1.32)     3416264.200  (0.51)     3387134.000  (-0.34)
splash2x/ocean_ncp      3947092.800  3939805.400  (-0.18)    3952311.600  (0.13)     7165858.800  (81.55)    4610075.000  (16.80)    3944753.400  (-0.06)
splash2x/radiosity      1475024.000  1474053.200  (-0.07)    1475032.400  (0.00)     1483718.800  (0.59)     1475919.600  (0.06)     99637.200    (-93.25)
splash2x/radix          2431302.200  2416928.600  (-0.59)    2455596.800  (1.00)     2568526.400  (5.64)     2479966.800  (2.00)     2437406.600  (0.25)
splash2x/raytrace       23274.400    23278.400    (0.02)     23287.200    (0.05)     28828.000    (23.86)    27800.200    (19.45)    5667.000     (-75.65)
splash2x/volrend        44106.800    44151.400    (0.10)     44186.000    (0.18)     45200.400    (2.48)     44751.200    (1.46)     16912.000    (-61.66)
splash2x/water_nsquared 29427.200    29425.600    (-0.01)    29402.400    (-0.08)    28055.400    (-4.66)    28572.400    (-2.90)    13207.800    (-55.12)
splash2x/water_spatial  664312.200   664095.600   (-0.03)    663025.200   (-0.19)    664100.600   (-0.03)    663597.400   (-0.11)    261214.200   (-60.68)
total                   29321300.000 29401500.000 (0.27)     29338300.000 (0.06)     33179900.000 (13.16)    30175600.000 (2.91)     23393600.000 (-20.22)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In total, 20.22% of residential sets were reduced.&lt;/p&gt;
&lt;p&gt;With parsec3/freqmine, &amp;lsquo;prcl&amp;rsquo; reduced 93.38% of residential sets and 23.63% of
system memory usage while incurring only 1.22% runtime overhead.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linux Kernel Performance (LKP) Tests</title>
      <link>https://sjp38.github.io/ko/post/lkp-tests/</link>
      <pubDate>Thu, 02 Aug 2018 17:20:10 +0900</pubDate>
      <guid>https://sjp38.github.io/ko/post/lkp-tests/</guid>
      <description>&lt;p&gt;리눅스 커널의 개발은 커뮤니티 주도입니다.  개발의 한 부분인 테스트 역시
커뮤니티 주도적입니다.  여러 개인 또는 단체가 커널을 각자의 방식으로 테스트
하고 그 결과를 공유합니다.  인텔에서는 0-day 서비스[1] 라는 서비스를 자체적으로
돌리는데, 이 서비스는 최신 리눅스 커널을 가져다가 빌드하고 다양한 기능 / 성능
테스트를 돌리고 그 결과 발견된 regression 을 LKML 에 메일로 보내주는 일을
합니다.  말하자면 Continuous Integration (CI) 이죠.&lt;/p&gt;
&lt;p&gt;Linux Kernel Performance (LKP) Tests[2] 는 0-day 서비스에서 기능 / 성능 테스트를
수행하는데 사용되는 도구입니다.  다양한 테스트를 돌리기 위한 시스템 환경 구성,
테스트 프로그램과 그 종속 프로그램 / 라이브러리의 설치와 환경 구성, 테스트
수행과 결과 정리, 그리고 테스트 진행 사이의 시스템 상태 프로파일링 및
프로파일링 결과 정리를 대신해 줍니다.  0-day 서비스에 연결되어 있지만 lkp-tests
와 0-day 서비스 사이의 종속성이 없으며, 오픈소스 프로젝트로 개발이 진행되고
있어 그 소스코드를 누구나 사용할 수 있으며 개인이 사용하기에도 편리하게 되어
있어서 개인 개발자가 자신의 패치를 테스트할 목적으로 사용하기에도 좋습니다.  이
글에서는 이러한 lkp-tests 의 구조와 사용법을 간단히 설명합니다.  전체적으로
인텔의 관련 블로그 글[3] 을 참고했습니다.&lt;/p&gt;
&lt;h1 id=&#34;설치&#34;&gt;설치&lt;/h1&gt;
&lt;p&gt;먼저 다음 커맨드로 lkp-tests 소스코드를 얻어옵니다:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/intel/lkp-tests
$ cd lkp-tests
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;그리고 lkp-tests 자체를 설치.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo make install
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;이 커맨드는 단순히 지금 소스코드를 땡겨온 lkp-tests 소스코드 디렉토리의
&lt;code&gt;bin/lkp&lt;/code&gt; 파일을 링크하는 &lt;code&gt;/usr/local/bin/lkp&lt;/code&gt; 심볼릭 링크를 만들 뿐입니다.
따라서 셸에서 &lt;code&gt;lkp&lt;/code&gt; 커맨드를 쓸 수 있게 해주죠.  이 &lt;code&gt;lkp&lt;/code&gt; 파일이 결국 lkp-tests
의 대부분의 일을 해주는 핵심 커맨드입니다.  이 프로그램의 간단한 사용법은
다음과 같이 확인할 수 있습니다:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ lkp
Usage: lkp &amp;lt;command&amp;gt; [options]

INSTALLATION

        install JOB                     install binary dependencies for JOB

JOB

        split JOB                       split JOB matrix
        compile JOB                     compile JOB into shell script

TESTING

        run JOB                         run test JOB locally
        qemu JOB                        run test JOB in QEMU virtual machine

RESULT

        result|rt|_rt|__rt PATTERNs     show result dirs
        ls|ll PATTERNs                  ls result dirs
        rm-path result                  remove result dirs
        _rm PATTERNs                    remove _result dirs

        stat [options]                  show result stats
        compare [options]               compare result stats

DEBUG

        irb                             run irb with lib/*.rb loaded
        pry                             run pry with lib/*.rb loaded

More commands can be found in /home/sjpark/lkp-tests/{bin,sbin,tools}/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;여기서 &lt;code&gt;&amp;lt;command&amp;gt;&lt;/code&gt; 는 lkp-tests 소스코드 디렉토리 아래 &lt;code&gt;bin/&lt;/code&gt;, &lt;code&gt;sbin/&lt;/code&gt;,
&lt;code&gt;tools/&lt;/code&gt;, 또는 &lt;code&gt;lkp-exec/&lt;/code&gt; 아래 위치한 실행파일로, &lt;code&gt;lkp&lt;/code&gt; 는 단순히 그
실행파일을 수행하면서 인자를 넘길 뿐입니다.&lt;/p&gt;
&lt;h1 id=&#34;테스트에-필요한-소프트웨어-설치&#34;&gt;테스트에 필요한 소프트웨어 설치&lt;/h1&gt;
&lt;p&gt;다음 커맨드는 lkp-tests 가 의존하고 있는 기본적 소프트웨어 패키지들을 모두
설치합니다:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo lkp install
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Ubuntu 16.04 에서는 다음 패키지들을 까는군요:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;bc gawk gzip kmod time automake bison bsdtar build-essential bzip2
ca-certificates cpio fakeroot flex gcc git libc6-dev libc6-dev:i386
libklibc-dev libtool linux-libc-dev linux-libc-dev:i386 linux-tools-generic
make openssl patch rsync ruby ruby-dev wget
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;lkp-tests-내부-벤치마크-실행&#34;&gt;lkp-tests 내부 벤치마크 실행&lt;/h1&gt;
&lt;p&gt;이제 lkp-tests 에서 지원하는 벤치마크를 실제로 돌려봅시다.&lt;/p&gt;
&lt;h2 id=&#34;테스트-수행에-필요한-환경-설정&#34;&gt;테스트 수행에 필요한 환경 설정&lt;/h2&gt;
&lt;p&gt;실제 테스트를 돌리기 위해선 벤치마크를 깔고, 그 벤치마크가 사용하는 패키지를
깔고, 테스트에 사용될 도구들을 깔고, 여러 설정을 하는등의 작업이 필요합니다.&lt;/p&gt;
&lt;p&gt;다음 명령은 lkp-tests 에서 ebizzy 라는 벤치마크를 사용하는 테스트를 위해
필요한 프로그램들을 설치하고 수행 환경을 준비합니다.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo lkp install jobs/ebizzy.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;job&#34;&gt;job&lt;/h2&gt;
&lt;p&gt;lkp-tests 에서 테스트 수행의 기본 단위는 &lt;code&gt;job&lt;/code&gt; 이라고 이야기 하는데, &lt;code&gt;jobs/&lt;/code&gt;
디렉토리 아래에 각 job 을 설명한 yaml 파일이 있습니다.  위의 커맨드에서 인자로
넣은 &lt;code&gt;ebizzy.yaml&lt;/code&gt; 은 ebizzy 를 사용한 테스트에 필요한 사항들이 들어있습니다.
이 커맨드는 내부적으로 이 테스트에 필요한 패키지를 설치하고 ebizzy 벤치마크도
소스코드를 인터넷에서 받아다가 컴파일해 설치합니다.&lt;/p&gt;
&lt;p&gt;Job 파일은 또한 해당 테스트가 어떤 시스템 환경들에서 어떤 벤치마크를 어떤
인자를 줘가며 수행해야할지 등에 대한 내용을 담고 있습니다.  예컨대
&lt;code&gt;jobs/aim7-fs-1brd.yaml&lt;/code&gt; job 은 &lt;code&gt;xfs&lt;/code&gt;, &lt;code&gt;ext4&lt;/code&gt;, &lt;code&gt;btrfs&lt;/code&gt;, &lt;code&gt;f2fs&lt;/code&gt; 파일 시스템
각각에 대해 aim7 을 돌려보도록 되어 있습니다.&lt;/p&gt;
&lt;h2 id=&#34;테스트-수행&#34;&gt;테스트 수행&lt;/h2&gt;
&lt;p&gt;다음 명령은 ebizzy job 으로 기술된 테스트를 실제 수행시킵니다:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo lkp run jobs/ebizzy.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;job-쪼개기&#34;&gt;job 쪼개기&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;ebizzy.yaml&lt;/code&gt; 은 시스템 소유 CPU 코어 갯수 2배 갯수의 쓰레드를 사용해 ebizzy 를
10초간 돌리는 실험을 100번 반복하도록 되어 있습니다.  즉, 실험의 경우의 수가
하나입니다.  그러나, 앞서 설명한 &lt;code&gt;aim7-fs-1brd&lt;/code&gt; 와 같이 다양한 구성을 사용하게
되어 있는 경우, 한가지 구성에 대해서만 반복 실험을 하고 싶을 수 있을 겁니다.
이런 경우 다음 명령으로 job 을 쪼갤 수 있습니다:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo lkp split jobs/ebizzy.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;이 커맨드는 인자로 넣은 job 파일을 쪼개서 현재 디렉토리에 쪼개진 job 파일들을
저장합니다.  어떤 구성이 쪼개졌는지는 파일 이름으로 볼 수 있습니다.
&lt;code&gt;ebizzy.yaml&lt;/code&gt; 을 쪼개면 &lt;code&gt;ebizzy-200%-100x-10s.yaml&lt;/code&gt; 라는 이름의 한개의 job
파일이 생성됩니다.  앞서 이야기한 &lt;code&gt;aim7-fs-1brd.yaml&lt;/code&gt; 을 쪼개면 다음과 같이
많은 수의 job 이 생성됩니다:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;aim7-fs-1brd-1BRD_48G-btrfs-creat-clo-4.yaml
aim7-fs-1brd-1BRD_48G-f2fs-creat-clo-1500.yaml
aim7-fs-1brd-1BRD_48G-btrfs-disk_cp-1500.yaml
aim7-fs-1brd-1BRD_48G-f2fs-disk_cp-3000.yaml
aim7-fs-1brd-1BRD_48G-btrfs-disk_rd-9000.yaml
aim7-fs-1brd-1BRD_48G-f2fs-disk_rd-9000.yaml
aim7-fs-1brd-1BRD_48G-btrfs-disk_rr-1500.yaml
aim7-fs-1brd-1BRD_48G-f2fs-disk_rr-3000.yaml
aim7-fs-1brd-1BRD_48G-btrfs-disk_rw-1500.yaml
aim7-fs-1brd-1BRD_48G-f2fs-disk_rw-3000.yaml
aim7-fs-1brd-1BRD_48G-btrfs-disk_src-500.yaml
aim7-fs-1brd-1BRD_48G-f2fs-disk_src-3000.yaml
aim7-fs-1brd-1BRD_48G-btrfs-disk_wrt-1500.yaml
aim7-fs-1brd-1BRD_48G-f2fs-disk_wrt-3000.yaml
aim7-fs-1brd-1BRD_48G-btrfs-sync_disk_rw-10.yaml
aim7-fs-1brd-1BRD_48G-f2fs-sync_disk_rw-600.yaml
aim7-fs-1brd-1BRD_48G-ext4-creat-clo-1000.yaml
aim7-fs-1brd-1BRD_48G-xfs-creat-clo-1500.yaml
aim7-fs-1brd-1BRD_48G-ext4-disk_cp-3000.yaml
aim7-fs-1brd-1BRD_48G-xfs-disk_cp-3000.yaml
aim7-fs-1brd-1BRD_48G-ext4-disk_rd-9000.yaml
aim7-fs-1brd-1BRD_48G-xfs-disk_rd-9000.yaml
aim7-fs-1brd-1BRD_48G-ext4-disk_rr-3000.yaml
aim7-fs-1brd-1BRD_48G-xfs-disk_rr-3000.yaml
aim7-fs-1brd-1BRD_48G-ext4-disk_rw-3000.yaml
aim7-fs-1brd-1BRD_48G-xfs-disk_rw-3000.yaml
aim7-fs-1brd-1BRD_48G-ext4-disk_src-3000.yaml
aim7-fs-1brd-1BRD_48G-xfs-disk_src-3000.yaml
aim7-fs-1brd-1BRD_48G-ext4-disk_wrt-3000.yaml
aim7-fs-1brd-1BRD_48G-xfs-disk_wrt-3000.yaml
aim7-fs-1brd-1BRD_48G-ext4-sync_disk_rw-600.yaml
aim7-fs-1brd-1BRD_48G-xfs-sync_disk_rw-600.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;테스트-결과-확인&#34;&gt;테스트 결과 확인&lt;/h1&gt;
&lt;p&gt;테스트에 사용된 벤치마크 수행의 결과와 테스트 동안의 시스템 프로파일링 결과는
&lt;code&gt;/lkp/result/&lt;/code&gt; 디렉토리 밑에 &lt;code&gt;&amp;lt;job name&amp;gt;/&amp;lt;configuration&amp;gt;/&amp;lt;host name&amp;gt;/&amp;lt;os distribution name&amp;gt;/&amp;lt;kernel config&amp;gt;/&amp;lt;gcc version&amp;gt;/&amp;lt;kernel version&amp;gt;/&amp;lt;unique id&amp;gt;&lt;/code&gt;
의 계층 구조로 저장됩니다.  또한, 이 실험 결과를 parsing 할 수 있는 경우엔
&lt;code&gt;.json&lt;/code&gt; 파일을 만들어 줘서 parsing 된 결과 값도 보여줍니다.&lt;/p&gt;
&lt;p&gt;또한, 최근의 테스트 결과는 테스트 돌린 디렉토리에 &lt;code&gt;result/&lt;/code&gt; 라는 이름의,
&lt;code&gt;/lkp/result/&lt;/code&gt; 아래 해당 디렉토리로의 심볼릭 링크를 만들어 줍니다.&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;이렇게 lkp-tests 가 무엇인지, 그리고 어떻게 사용할 수 있는지 알아봤습니다.
이제, 리눅스 커널 개발 커뮤니티에서 사용하는 리그레션 테스트를 여러분의
환경에서도 손쉽게 돌려볼 수 있습니다.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] &lt;a href=&#34;https://01.org/lkp/documentation/0-day-test-service&#34;&gt;https://01.org/lkp/documentation/0-day-test-service&lt;/a&gt;&lt;br&gt;
[2] &lt;a href=&#34;https://github.com/intel/lkp-tests&#34;&gt;https://github.com/intel/lkp-tests&lt;/a&gt;&lt;br&gt;
[3] &lt;a href=&#34;https://01.org/blogs/jdu1/2017/lkp-tests-linux-kernel-performance-test-and-analysis-tool&#34;&gt;https://01.org/blogs/jdu1/2017/lkp-tests-linux-kernel-performance-test-and-analysis-tool&lt;/a&gt;&lt;br&gt;
[4] &lt;a href=&#34;https://wiki.archlinux.org/index.php/makepkg&#34;&gt;https://wiki.archlinux.org/index.php/makepkg&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
