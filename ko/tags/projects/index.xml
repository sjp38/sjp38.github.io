<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>projects | hacklog</title>
    <link>/ko/tags/projects/</link>
      <atom:link href="/ko/tags/projects/index.xml" rel="self" type="application/rss+xml" />
    <description>projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>ko-kr</language><lastBuildDate>Fri, 27 Dec 2019 18:21:07 +0100</lastBuildDate>
    <image>
      <url>/img/tux.png</url>
      <title>projects</title>
      <link>/ko/tags/projects/</link>
    </image>
    
    <item>
      <title>DAMON: Data Access Monitor</title>
      <link>/ko/post/damon/</link>
      <pubDate>Fri, 27 Dec 2019 18:21:07 +0100</pubDate>
      <guid>/ko/post/damon/</guid>
      <description>&lt;p&gt;Modern workloads tend to have huge working sets and low locality.  Despite this
trend, the capacity of DRAM has not been increased enough to accommodate such
huge working sets. Therefore, memory management mechanisms optimized for such
modern workloads are widely required today. For such optimizations, knowing the
data access pattern of given workloads is essential. However, manually
extracting such patterns from huge and complex workloads is exhaustive. Worse
yet, existing memory access analysis tools incur unacceptably high overheads
for unnecessarily detailed analysis results.&lt;/p&gt;
&lt;p&gt;To mitigate this situation, we introduce a tool that is designed for data
access pattern tracing. Two core mechanisms in this tool, a region-based
sampling and an adaptive region adjustment, allow users to limit the tracing
overhead in a bounded range regardless of the size and complexity of target
workloads, while preserving the quality of results. Our empirical evaluations
that conducted with 20 realistic workloads show the high quality, low overhead,
and a potential use case of this tool.&lt;/p&gt;
&lt;h2 id=&#34;news&#34;&gt;News&lt;/h2&gt;
&lt;p&gt;RFC v1 of DAMON has introduced by LWN&#39;s &amp;lsquo;Kernel patches of interest&amp;rsquo;:
&lt;a href=&#34;https://lwn.net/Articles/809100/&#34;&gt;https://lwn.net/Articles/809100/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The first RFC patchset has posted to LKML:
&lt;a href=&#34;https://lore.kernel.org/linux-mm/20200110131522.29964-1-sjpark@amazon.com/&#34;&gt;https://lore.kernel.org/linux-mm/20200110131522.29964-1-sjpark@amazon.com/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;publications-and-presentations&#34;&gt;Publications and Presentations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;SeongJae Park, Yunjae Lee, Heon Y. Yeom, &lt;strong&gt;Profiling Dynamic Data Access
Patterns with Controlled Overhead and Quality.&lt;/strong&gt; In 20th ACM/IFIP
International Middleware Conference Industry, December 2019.
&lt;a href=&#34;https://dl.acm.org/citation.cfm?id=3368125&#34;&gt;Link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SeongJae Park, &lt;strong&gt;Tracing Data Access Pattern with Bounded Overhead and
Best-effort Accuracy.&lt;/strong&gt; In &lt;em&gt;The Linux Kernel Summit&lt;/em&gt;, September 2019.
&lt;a href=&#34;https://linuxplumbersconf.org/event/4/contributions/548/attachments/311/590/damon_ksummit19.pdf&#34;&gt;Slides&lt;/a&gt;,
&lt;a href=&#34;https://linuxplumbersconf.org/event/4/contributions/548/&#34;&gt;Link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SeongJae Park, Yunjae Lee, Yunhee Kim, Heon Y. Yeom, &lt;strong&gt;Profiling Dynamic Data
Access Patterns with Bounded Overhead and Accuracy.&lt;/strong&gt; In IEEE International
Workshop on Foundations and Applications of Self-* Systems (FAS* 2019),
June 2019.
&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/8791992&#34;&gt;Link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>DAPHICX: Data Access Pattern Hint Injecting Compiler Extension</title>
      <link>/ko/post/daphicx/</link>
      <pubDate>Thu, 30 May 2019 18:59:53 +0900</pubDate>
      <guid>/ko/post/daphicx/</guid>
      <description>&lt;p&gt;Memory pressure is inevitable as the size of working sets is rapidly growing
while the capacity of dynamic random access memory (DRAM) is not. Meanwhile,
storage devices have evolved so that their speed is comparable to the speed of
DRAM while their capacity scales are comparable to that of hard disk drives
(HDD). Thus, hierarchial memory systems configuring DRAM as the main memory and
high-end storages as swap devices will be common.&lt;/p&gt;
&lt;p&gt;Due to the unique characteristics of these modern storage devices, the swap
target decision should be optimal. It is essential to know the exact data
access patterns of workloads for such an optimal decision, although underlying
systems cannot accurately estimate such complex and dynamic patterns. For this
reason, memory systems allow programs to voluntarily hint their data access
pattern. Nevertheless, it is exhausting for a human to manually figure out the
patterns and embed optimal hints if the workloads are huge and complex.&lt;/p&gt;
&lt;p&gt;This project introduces a compiler extension that automatically optimizes a
program to voluntarily hint its dynamic data access patterns to the underlying
swap system using a static/dynamic analysis based profiling result. To our best
knowledge, this is the first profile-guided optimization (PGO) for modern swap
devices. Our empirical evaluation of the scheme using realistic workloads shows
consistent improvement in performance and swap device lifetime up to 2.65 times
and 2.98 times, respectively.&lt;/p&gt;
&lt;h1 id=&#34;publications-and-presentations&#34;&gt;Publications And Presentations&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;SeongJae Park, Yunjae Lee, Moonsub Kim Heon Y. Yeom, &lt;strong&gt;Automating Context
Based Access Pattern Hint Injection for System Performance and Swap Storage
Durability.&lt;/strong&gt; In 11th USENIX Workshop on Hot Topics in Storage and File
Systems (HotStorage), July 2019.
&lt;a href=&#34;https://www.usenix.org/system/files/hotstorage19-paper-park.pdf&#34;&gt;Paper&lt;/a&gt;,
&lt;a href=&#34;https://www.usenix.org/sites/default/files/conference/protected-files/hotstorage19_slides_park.pdf&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SeongJae Park, Yunjae Lee, Moonsub Kim, Heon Y. Yeom, &lt;strong&gt;Automated Data Access
Pattern Hint Instrumentation for System Performance and Durability of Swap
Storages.&lt;/strong&gt; (WiP) In 17th USENIX Conference on File and Storage Technologies
(FAST), February 2019.
&lt;a href=&#34;https://www.usenix.org/conference/fast19/wips&#34;&gt;Link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Idle Page Tracking Tools</title>
      <link>/ko/post/idle_page_tracking/</link>
      <pubDate>Wed, 13 Sep 2017 13:46:00 +0900</pubDate>
      <guid>/ko/post/idle_page_tracking/</guid>
      <description>&lt;p&gt;&lt;code&gt;idle_page_tracking&lt;/code&gt;[1] is a simple, stupid toolbox for idle pages tracking.
It can be used to get real working set size of a process.&lt;/p&gt;
&lt;h1 id=&#34;tools&#34;&gt;Tools&lt;/h1&gt;
&lt;p&gt;This section describes two tools in the box though more tools exists.  You can
get more description about each tool from the README in the repository[1].&lt;/p&gt;
&lt;h2 id=&#34;userprog&#34;&gt;userprog&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;userprog&lt;/code&gt; is a sample synthetic workload for test of other tools.  It
interactively allocates and access specified pages in the allocated pages.
After execution, it first asks how many pages to allocates.  Once you type in
how many pages to allocate, the program will repeatedly asks how many pages in
the allocated pages you want to do access.&lt;/p&gt;
&lt;h2 id=&#34;wspagessh&#34;&gt;wspages.sh&lt;/h2&gt;
&lt;p&gt;Now you can calculate working set size of a process using the tools.  To
simplify the life even more, &lt;code&gt;wspages.sh&lt;/code&gt; helps the complicated works.  It
requires pid, time interval, and target memory mapped regions as argument.  The
third argument can be ignored.  In the case, it uses heap, stack, and anonymous
pages as target memory region by default.  If you give the arguments well, this
tool will prints out number of pages accessed between the time interval.
Simple example of usage and output is as below:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo ./wspages.sh `pidof userprog` 1 [heap]
3
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;wspgstatsh&#34;&gt;wspgstat.sh&lt;/h2&gt;
&lt;p&gt;Like &lt;code&gt;*stat&lt;/code&gt; programs (such as vmstat, iostat, &amp;hellip;), wspgstat.sh monitors and
print out number of pages in working set of specific process repeatedly.  It
requires pid of target program, delay between idleness check, and target memory
mapped regions as arguments.  The third argument is optional and has default
value as same as wspages.sh&#39;s same argument.  Simple example usage is as below:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ ./wspgstat.sh `pidof mysqld` 5
1 17448
9 21536
18 21659
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;limitations&#34;&gt;Limitations&lt;/h1&gt;
&lt;p&gt;The tools use idle page tracking feature of the Linux kernel[2] internally.  It
means that the tools work on Linux systems that idle page tracking feature is
turned on.  You can check whether your system turned on or off the feature by
simply running the command below:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ if [ -d /sys/kernel/mm/page_idle ]; \
        then echo &amp;quot;ON&amp;quot;; else echo &amp;quot;OFF&amp;quot;; fi
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;It also shares limitation of idle page tracking feature of the Linux kernel.
It tracks only userspace pages on LRU list of the kernel.&lt;/p&gt;
&lt;h1 id=&#34;license&#34;&gt;License&lt;/h1&gt;
&lt;p&gt;GPL v3&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;p&gt;[1] &lt;a href=&#34;https://github.com/sjp38/idle_page_tracking&#34;&gt;https://github.com/sjp38/idle_page_tracking&lt;/a&gt;&lt;br&gt;
[2] &lt;a href=&#34;https://www.kernel.org/doc/Documentation/vm/idle_page_tracking.txt&#34;&gt;https://www.kernel.org/doc/Documentation/vm/idle_page_tracking.txt&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>lkml livestream</title>
      <link>/ko/post/lkml_go/</link>
      <pubDate>Sun, 28 May 2017 17:51:43 +0900</pubDate>
      <guid>/ko/post/lkml_go/</guid>
      <description>&lt;p&gt;쏟아지는 LKML[2] 의 메일들을 트위터 라이브스트림처럼 터미널에 보여주는 간단한
프로그램[1] 을 go 언어로 만들어 봤습니다.  아직 보완할 점 투성이지만 이제
최초의 목적대로는 동작하는군요.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;p&gt;[1] &lt;a href=&#34;https://github.com/sjp38/lkml&#34;&gt;https://github.com/sjp38/lkml&lt;/a&gt;&lt;br&gt;
[2] &lt;a href=&#34;https://en.wikipedia.org/wiki/Linux_kernel_mailing_list&#34;&gt;https://en.wikipedia.org/wiki/Linux_kernel_mailing_list&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GCMA: Guaranteed Contiguous Memory Allocator</title>
      <link>/ko/post/gcma/</link>
      <pubDate>Sat, 14 Jan 2017 23:11:23 +0900</pubDate>
      <guid>/ko/post/gcma/</guid>
      <description>&lt;p&gt;The importance of physically contiguous memory has increased in modern
computing environments, including both low- and high-end systems. Existing
physically contiguous memory allocators generally have critical limitations.
For example, the most commonly adopted solution, the memory reservation
technique, wastes a significant amount of memory space. Scatter/Gather direct
memory access (DMA) and input-output memory management units (IOMMUs) avoid
this problem by utilizing additional hardware for address space virtualization.
However, additional hardware means an increase in costs and power consumption,
which is especially disadvantageous for small systems and they do not provide
real contiguous memory. Linux Contiguous Memory Allocator (CMA) aims to provide
both contiguous memory allocation and to maximize memory utilization based on
page migration, but they suffer from unpredictably long latency and a high
probability of allocation failure. Therefore, we introduce a new solution to
this problem, the guaranteed contiguous memory allocator (GCMA). This
guarantees efficient memory space utilization, short latency, and successful
allocation. The GCMA uses a reservation scheme and increases memory utilization
by sharing the memory with immediately discardable data. Our evaluation of a
GCMA on a Raspberry Pi 2 finds a latency that is 15-130 times lower compared to
a CMA, and a latency that is up to 10 times lower when taking a photo. Using a
large working set in a memory-fragmented high-end system, the GCMA is able to
produce a 2.27× speedup.&lt;/p&gt;
&lt;h1 id=&#34;source-code&#34;&gt;Source Code&lt;/h1&gt;
&lt;p&gt;The source code for this version has been submitted to &lt;a href=&#34;https://lkml.org/lkml/2015/2/23/480&#34;&gt;LKML&lt;/a&gt; for discussion.
A complete git tree is also available at &lt;a href=&#34;https://github.com/sjp38/linux.gcma&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;publications-and-presentations&#34;&gt;Publications and Presentations&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;SeongJae Park, Minchan Kim, Heon Y. Yeom, &lt;strong&gt;GCMA: Guaranteed Contiguous
Memory Allocator.&lt;/strong&gt; In &lt;em&gt;Transactions on Computers&lt;/em&gt;, March 2019.
&lt;a href=&#34;https://ieeexplore.ieee.org/document/8456561&#34;&gt;Link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SeongJae Park, &lt;strong&gt;GCMA: Guaranteed Contiguous Memory Allocator.&lt;/strong&gt; In &lt;em&gt;The
Linux Kernel Summit&lt;/em&gt;, November 2018.
&lt;a href=&#34;https://linuxplumbersconf.org/event/2/contributions/247/attachments/74/85/gcma_ksummit2018.pdf&#34;&gt;Slides&lt;/a&gt;,
&lt;a href=&#34;https://www.youtube.com/watch?v=ARrelFfdVkw&#34;&gt;Video&lt;/a&gt;,
&lt;a href=&#34;https://linuxplumbersconf.org/event/2/contributions/247/&#34;&gt;Link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SeongJae Park, Minchan Kim, Heon Y. Yeom, &lt;strong&gt;GCMA: Guaranteed Contiguous
Memory Allocator.&lt;/strong&gt; In &lt;em&gt;45th issue of ACM SIGBED Review&lt;/em&gt;, January 2016.
&lt;a href=&#34;http://sigbed.seas.upenn.edu/archives/2016-01/EWiLi15_4.pdf&#34;&gt;Paper&lt;/a&gt;,
&lt;a href=&#34;http://sigbed.seas.upenn.edu/vol13_num1.html#issue&#34;&gt;Link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SeongJae Park, Minchan Kim, Heon Y. Yeom, &lt;strong&gt;GCMA: Guaranteed Contiguous
Memory Allocator.&lt;/strong&gt; In &lt;em&gt;Embedded Operating Systems Workshop (EWiLi)&lt;/em&gt;, October
2015.
&lt;a href=&#34;http://ceur-ws.org/Vol-1464/ewili15_12.pdf&#34;&gt;Paper&lt;/a&gt;,
&lt;a href=&#34;https://www.slideshare.net/SeongJaePark1/gcma-guaranteed-contiguous-memory-allocator&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SeongJae Park, Minchan Kim, &lt;strong&gt;GCMA: Guaranteed Contiguous Memory Allocator.&lt;/strong&gt;
In &lt;em&gt;Linux Foundation Korea Linux Forum (LFKLF)&lt;/em&gt;, October 2014.
&lt;a href=&#34;http://events.linuxfoundation.org/sites/events/files/slides/gcma-guaranteed_contiguous_memory_allocator-lfklf2014_0.pdf&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>&#34;Is Parallel Programming Hard, And, If So, What Can You Do About It?&#34; Translation</title>
      <link>/ko/post/perfbook-kokr/</link>
      <pubDate>Tue, 10 Jan 2017 13:46:00 +0900</pubDate>
      <guid>/ko/post/perfbook-kokr/</guid>
      <description>&lt;p&gt;&amp;ldquo;Is Parallel Programming is Hard, And, If So, What Can You Do About It?&amp;quot;[1] 은
parallel programming 분야에서 대가라 불릴만한 분으로, 이쪽 분야에서 매우 중요한
동기화 메커니즘인 RCU[2] 를 개발했으며 리눅스 커널의 RCU 메인테이너로 활동하고
있는 Paul E. McKenney[3] 가 오픈소스 방식으로 저술하고 있는, parallel
programming 에 대한 책입니다.&lt;/p&gt;
&lt;p&gt;개인적으로 이 책의 한국어 번역을 오픈소스[4]로 진행하고 있습니다.  이
프로젝트는 원저작자인 Paul 에게 공식 한국어 번역으로 인증받았습니다[5].&lt;/p&gt;
&lt;p&gt;컨트리뷰션에 대해서도 열려 있으니, 이에 관심 있는 분은 repository 내의 README
문서의 Contribution 섹션[6] 을 참고 바랍니다.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;p&gt;[1] &lt;a href=&#34;https://www.kernel.org/pub/linux/kernel/people/paulmck/perfbook/perfbook.html&#34;&gt;https://www.kernel.org/pub/linux/kernel/people/paulmck/perfbook/perfbook.html&lt;/a&gt;&lt;br&gt;
[2] &lt;a href=&#34;https://en.wikipedia.org/wiki/Read-copy-update&#34;&gt;https://en.wikipedia.org/wiki/Read-copy-update&lt;/a&gt;&lt;br&gt;
[3] &lt;a href=&#34;http://www.rdrop.com/~paulmck/&#34;&gt;http://www.rdrop.com/~paulmck/&lt;/a&gt;&lt;br&gt;
[4] &lt;a href=&#34;https://github.com/sjp38/perfbook-ko_KR&#34;&gt;https://github.com/sjp38/perfbook-ko_KR&lt;/a&gt;&lt;br&gt;
[5] &lt;a href=&#34;https://git.kernel.org/pub/scm/linux/kernel/git/paulmck/perfbook.git/commit/?id=edbfcdee046026d3f98592c411a20219b96c8e50&#34;&gt;https://git.kernel.org/pub/scm/linux/kernel/git/paulmck/perfbook.git/commit/?id=edbfcdee046026d3f98592c411a20219b96c8e50&lt;/a&gt;&lt;br&gt;
[6] &lt;a href=&#34;https://github.com/sjp38/perfbook-ko_KR#contribution&#34;&gt;https://github.com/sjp38/perfbook-ko_KR#contribution&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
